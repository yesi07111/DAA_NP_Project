\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage[backend=biber,style=ieee]{biblatex}
\usepackage{csquotes}
\usepackage[spanish]{babel}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{mathptmx}
\usepackage{courier}
\usepackage{hyperref}
\usepackage{algorithm2e}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{thmtools}

\theoremstyle{definition}
\newtheorem{theorem}{Teorema}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{definition}[theorem]{Definición}
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{corollary}[theorem]{Corolario}

\geometry{letterpaper, margin=1in}
\DefineBibliographyStrings{spanish}{
    references = {Referencias},
    }
\addbibresource{referencias.bib}
\onehalfspacing

\title{\textbf{Proyecto DAA: Problema de la Empresa Telefónica - Partición Cromática de Costo Mínimo (MCCPP)}}
\author{
Yesenia Valdés Rodríguez (C411)\\
Laura Martir Beltrán (C411)\\
Adrián Hernández Castellanos (C412)
}
\date{}

\begin{document}

\maketitle
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage

\setcounter{page}{1}

\section{Formalización del Problema}

\subsection{Problema: La empresa Telefónica}
En ConectaMax Telecom, nuestra misión es proporcionar una conectividad móvil ininterrumpida y de alta calidad a millones de usuarios. Para lograrlo, operamos una extensa y compleja red de torres de telefonía celular. La eficiencia y la calidad de nuestra red dependen críticamente de cómo gestionamos uno de nuestros recursos más valiosos: el espectro de radiofrecuencias.

Nos enfrentamos a un desafío operativo y financiero significativo en la asignación de frecuencias a nuestras torres. Disponemos de un conjunto limitado de frecuencias que podemos utilizar. La regla fundamental, dictada por la física y la regulación, es que dos torres que están geográficamente muy cerca una de la otra (y que, por lo tanto, podrían interferir entre sí) no pueden operar en la misma frecuencia. Esto es vital para evitar la degradación de la señal y asegurar un servicio fiable.

La complejidad adicional, y donde reside nuestro mayor reto, es que el costo de operar una torre con una frecuencia específica no es uniforme. Asignar una frecuencia particular a una torre determinada conlleva costos variables. Estos costos pueden deberse a múltiples factores:

\begin{itemize}
\item \textbf{Equipamiento de la Torre:} Algunas torres tienen hardware más antiguo o especializado que es más eficiente con ciertas frecuencias, mientras que otras frecuencias podrían requerir adaptaciones o un mayor consumo energético en ese mismo equipo.
\item \textbf{Consumo Energético:} La eficiencia energética de la transmisión varía según la frecuencia y el tipo de equipo de la torre, impactando directamente nuestras facturas de electricidad.
\item \textbf{Regulaciones Locales y Licencias:} En ciertas áreas o para bandas de frecuencia específicas, pueden existir tarifas de licencia más elevadas o regulaciones que exigen configuraciones especiales, aumentando los costos operativos.
\end{itemize}

Nuestro objetivo principal es asignar una frecuencia a cada una de nuestras torres de tal manera que:

\begin{enumerate}
\item \textbf{Se eviten todas las interferencias:} Ninguna torre cercana a otra utilice la misma frecuencia.
\item \textbf{Se minimice el costo operativo total:} La suma de los costos individuales de asignar cada frecuencia a cada torre sea la más baja posible.
\end{enumerate}

Una gestión subóptima de esta asignación no solo puede generar interferencias que afectan la calidad del servicio y la satisfacción del cliente, sino que también puede resultar en millones de dólares en costos operativos innecesarios.

\subsection{Contexto del Problema: La Asignación de Frecuencias en ConectaMax Telecom}

El problema presentado por la empresa ConectaMax Telecom, centrado en la asignación óptima de frecuencias a sus torres de telefonía celular, se traduce directamente en un problema de optimización combinatoria conocido como el \textbf{Problema de Partición Cromática de Costo Mínimo (Minimum Cost Chromatic Partition Problem, MCCPP)}.

La estructura de la red telefónica y sus restricciones operativas se modelan de la siguiente manera:

\begin{itemize}
    \item \textbf{Grafo de Interferencia $G=(V, E)$}: Las Torres de Telefonía Celular se representan como los vértices ($V$) del grafo. Las aristas ($E$) representan las proximidades geográficas y el potencial de interferencia, donde una arista $\{u, v\} \in E$ indica que la Torre $u$ y la Torre $v$ no pueden operar en la misma frecuencia.
    
    \item \textbf{Etiquetas y Frecuencias $F$}: El conjunto limitado de frecuencias disponibles se convierte en el conjunto finito de etiquetas ($F$) (o colores).
    
    \item \textbf{Restricción Operacional (Coloración Propia)}: La regla de “evitar todas las interferencias” se traduce en la restricción de coloración propia, exigiendo que dos vértices adyacentes no posean la misma etiqueta:
    \[
        \phi(u) \neq \phi(v)
    \]
    
    \item \textbf{Costo Operativo $C$}: La variabilidad en el costo de operar una torre con una frecuencia específica (debido a equipamiento, energía, licencias) se captura mediante la matriz de costos $C$, donde $c_{v,f}$ es el costo de asignar la frecuencia $f$ a la torre $v$.
    
    \item \textbf{Objetivo (Minimización del Costo)}: El objetivo de minimizar el costo operativo total se convierte en:
    \[
        \min \sum_{v \in V} c_{v, \phi(v)}
    \]
\end{itemize}

El MCCPP es, por lo tanto, la formalización precisa del desafío de la empresa telefónica, ya que busca una partición válida de los vértices (torres) en clases de color (frecuencias) que minimice el costo total asociado a dicha partición.

\subsection{Definición Formal}

\textbf{Nombre del Problema}: Problema de Partición Cromática de Costo Mínimo (MCCPP).

\textbf{Instancia de Entrada}: Una tupla $\langle G, F, C \rangle$, donde:

\begin{itemize}
    \item $G = (V, E)$: Un grafo no dirigido, con $|V|=n$ y $E$ el conjunto de aristas.
    \item $F$: Conjunto finito de etiquetas (colores/frecuencias), $|F|=k$.
    \item $C$: Matriz de costos donde $c_{v,f}$ es el costo de asignar la etiqueta $f$ al vértice $v$.
\end{itemize}

\textbf{Función Objetivo}: Encontrar una función de asignación de etiquetas $\phi: V \to F$ que minimice el costo total de la asignación.

\textbf{Restricción de Factibilidad}:

\begin{itemize}
\item Para toda arista $\{u, v\} \in E$, se debe cumplir que $\phi(u) \neq \phi(v) \quad$.
\end{itemize}

\subsection{Modelado de Programación Entera Lineal (ILP 0-1)}

El MCCPP se formaliza rigurosamente mediante el siguiente modelo de Programación Entera Lineal con variables binarias, que servirá de base para la demostración de complejidad y para la implementación de un algoritmo exacto (fuerza bruta/Branch-and-Bound en instancias pequeñas).

\textbf{Parámetros:}
\begin{itemize}
    \item $V$: Conjunto de vértices.
    \item $F$: Conjunto de etiquetas disponibles.
    \item $c_{v, f}$: Costo de asignar la etiqueta $f \in F$ al vértice $v \in V$.
\end{itemize}

\textbf{Variables de Decisión:}
\begin{itemize}
    \item $x_{v, f} \in \{0, 1\}$: Variable binaria. $x_{v, f} = 1$ si el vértice $v$ es asignado a la etiqueta $f$; $x_{v, f} = 0$ en caso contrario.
\end{itemize}

\textbf{Función Objetivo (Minimización del Costo Total):}
\[
\min \sum_{v \in V} \sum_{f \in F} c_{v, f} \cdot x_{v, f}
\]

\textbf{Restricciones de Factibilidad:}

\textbf{Asignación Única por Vértice:} Cada vértice debe ser asignado a exactamente una etiqueta.
\[
\sum_{f \in F} x_{v, f} = 1 \quad \forall v \in V
\]

\textbf{Restricción de No Conflicto (Coloración Propia):} Dos vértices adyacentes no pueden recibir la misma etiqueta.
\[
x_{u, f} + x_{v, f} \le 1 \quad \forall \{u, v\} \in E,\ \forall f \in F
\]

\textbf{Integridad:}
\[
x_{v, f} \in \{0, 1\} \quad \forall v \in V,\ \forall f \in F
\]

\section{Análisis de Complejidad Computacional}

\subsection{Clase de Complejidad}

El Problema de Partición Cromática de Costo Mínimo (MCCPP) es un problema NP-Hard en su versión de optimización. Dado que su restricción de factibilidad es idéntica a la del problema \(k\)-Colorability (reconocido como NP-Completo), la búsqueda de la solución óptima de costo mínimo es, por lo menos, tan difícil como la búsqueda de cualquier coloración válida.

\subsection{Planteamiento del Problema de \(k\)-Colorabilidad}

El \textbf{problema de \(k\)-Colorabilidad} (también conocido como \(k\)-Coloración Propia) plantea la siguiente cuestión fundamental: dado un grafo no dirigido \(G = (V, E)\) y un entero positivo \(k\), ¿es posible asignar colores a los vértices de \(G\) utilizando a lo sumo \(k\) colores diferentes, de tal forma que ningún par de vértices adyacentes compartan el mismo color?

Este problema representa uno de los problemas clásicos de NP-Completitud y tiene aplicaciones en áreas como \textit{scheduling}, asignación de registros en compiladores, y problemas de asignación de frecuencias en redes.

\subsection{Demostración de NP-Hardness mediante Reducción Polinomial}

Se demuestra que el MCCPP es NP-Hard mediante una reducción en tiempo polinomial del problema de decisión \(k\)-Colorability (una instancia canónica de NP-Completo) al MCCPP.

\textbf{Problema Fuente (NP-Completo):} \(k\)-Colorability.

\textbf{Instancia:} \(\langle G, k \rangle\), donde \(G=(V, E)\) es un grafo y \(k\) es un entero positivo.

\textbf{Pregunta:} ¿Existe una coloración propia de \(G\) utilizando a lo sumo \(k\) colores?

\textbf{Reducción Polinomial \(\mathcal{R}\):}

Dada una instancia \(\langle G, k \rangle\) del \(k\)-Colorability, construimos la siguiente instancia \(\langle G', F, C \rangle\) del MCCPP:

\begin{itemize}

\item \textbf{Grafo \(G'\):} Mantenemos el mismo grafo: \(G' = G\).

\item \textbf{Conjunto de Etiquetas \(F\):} Definimos el conjunto de etiquetas como \(F = \{f_1, f_2, \ldots, f_k\}\), con \(|F|=k\).

\item \textbf{Matriz de Costos \(C\):} Definimos un costo uniforme y nulo:
\[
c_{v, f} = 0 \quad \forall v \in V, \forall f \in F
\]

\item \textbf{Umbral de Decisión \(Z_{\max}\):} Establecemos el objetivo de optimización: \(\min Z\). La pregunta de decisión asociada al MCCPP será si el costo mínimo \(Z^*\) es \(\le 0\).

\end{itemize}

\textbf{Conclusión de la Reducción:}

\begin{itemize}

\item Si \(G\) es \(k\)-coloreable, existe una asignación \(\phi: V \to F\) que satisface la restricción de coloración propia. Como todos los costos son cero (\(c_{v, f} = 0\)), el costo total de esta asignación es \(Z = \sum c_{v, \phi(v)} = 0\). Por lo tanto, el costo mínimo óptimo \(Z^*\) es \(0\), y \(Z^* \le 0\).

\item Si \(G\) no es \(k\)-coloreable, no existe ninguna asignación válida \(\phi: V \to F\) que cumpla la restricción. Por lo tanto, no existe una solución factible, y no podemos alcanzar el costo \(Z \le 0\).

\end{itemize}

Dado que se ha reducido el \(k\)-Colorability al MCCPP en tiempo polinomial (la construcción solo requiere asignar \(k \cdot n\) costos a cero), y la respuesta a la instancia del MCCPP determina la respuesta a la instancia del \(k\)-Colorability, el MCCPP (en su versión de optimización) es NP-Hard.

\subsection{Jerarquía de Aproximabilidad Polinomial: PTAS, APX, APX-Hardness, APX-Complete y log-APX}

Dada la \textbf{NP-Hardness} del Problema de Partición Cromática de Costo Mínimo (MCCPP), el análisis de su \textbf{aproximabilidad} es crucial. Este análisis establece los límites teóricos sobre cuán cerca de la solución óptima \(Z^*\) podemos esperar llegar en tiempo polinomial. Para ello, es indispensable definir la jerarquía de las clases de complejidad de aproximación.

\begin{itemize}

\item \textbf{PTAS (Polynomial Time Approximation Scheme)}: La clase \textbf{PTAS} incluye los problemas de optimización que admiten un esquema de aproximación en tiempo polinomial. Esto significa que, para cualquier constante \(\epsilon > 0\) arbitrariamente pequeña, existe un algoritmo en tiempo polinomial \(O(n^{f(1/\epsilon)})\) que produce una solución \(Z_{Alg}\) tal que \(Z_{Alg} \le (1+\epsilon)Z^*\) (para problemas de minimización). La complejidad temporal puede crecer exponencialmente con \(1/\epsilon\), pero es polinomial en el tamaño de la entrada \(n\).

\item \textbf{APX (Approximable)}: La clase \textbf{APX} (Approximable) contiene todos los problemas NPO (NP Optimization) que admiten un algoritmo de aproximación de factor constante \(c\) en tiempo polinomial, donde \(c \ge 1\) es independiente del tamaño de la instancia \(n\). Formalmente, \(Z_{Alg} \le c \cdot Z^*\). La clase \textbf{PTAS} es un subconjunto estricto de \textbf{APX}, asumiendo que \(P \neq NP\). Pertenecer a APX implica que la calidad de la solución está acotada por una constante.

\item \textbf{APX-Hard}: Un problema de optimización es \textbf{APX-Hard} si todo problema en \textbf{APX} puede ser reducido a él mediante una reducción que preserva la aproximación (típicamente una L-Reducción o PTAS-Reducción). La consecuencia fundamental de la APX-Hardness es que, si \(P \neq NP\), el problema \textbf{no admite un PTAS}. La demostración de APX-Hardness del MCCPP (Sección 2.5) establece precisamente esta limitación.

\item \textbf{APX-Complete}: Un problema es \textbf{APX-Complete} si es \textbf{APX-Hard} y, además, pertenece a la clase \textbf{APX}. Esto implica que existe un algoritmo de aproximación de factor constante \(c\), pero que no existe un esquema de aproximación arbitrariamente cercano al óptimo (PTAS), salvo que \(P=NP\).

\item \textbf{log-APX}: Esta clase es relevante para problemas cuya garantía de aproximación es logarítmica. Un problema se clasifica en \textbf{log-APX} si admite un algoritmo de aproximación con un factor de rendimiento \(R\) acotado por \(O(\ln n)\), donde \(n\) es el tamaño de la entrada. La clasificación en \(\mathbf{log}\)-APX tiene una implicación más restrictiva que la APX-Hardness: los problemas en \(\mathbf{log}\)-APX (y que no están en APX) \textbf{no admiten un algoritmo de aproximación de factor constante} \(c\), a menos que se cumplan condiciones de complejidad teórica muy improbables.

\end{itemize}

\subsection{Demostración de APX-Hardness del MCCPP}

Para demostrar que un problema es APX-Hard, se requiere establecer una reducción que preserva la aproximación (típicamente una L-Reducción) desde un problema que ya se sabe es APX-Hard. Esta demostración refuta la posibilidad de esquemas de aproximación arbitrariamente cercanos al óptimo (PTAS), a menos que \(P=NP\).

Por lo tanto, se establecerá una L-Reducción robusta desde el Minimum Vertex Cover (MVC).

\subsubsection{Problema de Origen APX-Complete: Minimum Vertex Cover (MVC)}

El problema de Minimum Vertex Cover (MVC) es un problema de optimización de minimización: dada una instancia de grafo \(G=(V, E)\), se busca el subconjunto de vértices \(V' \subseteq V\) de tamaño mínimo tal que cada arista en \(E\) tenga al menos un extremo en \(V'\).

El MVC es un problema fundamental en complejidad y se sabe que es \textbf{APX-Complete} \cite{PY91}. Esto implica dos hechos cruciales:

\begin{enumerate}

\item \textbf{APX-Hardness}: MVC no admite un PTAS a menos que \(P=NP\) \cite{ALM+98}.

\item \textbf{Pertenencia a APX}: Existe un algoritmo de aproximación en tiempo polinomial con un factor de aproximación constante (una \(2\)-aproximación basada en un algoritmo voraz clásico).

\end{enumerate}

La demostración formal de que MVC es APX-Complete, confirmando su \(2\)-aproximabilidad y su inaproximabilidad constante (cota inferior de \(1.3606\) si \(P \neq NP\)), se puede encontrar, por ejemplo, en \cite{dinur2005vc}, así como en desarrollos posteriores sobre la brecha de aproximación de Vertex Cover.

El MVC está intrínsecamente ligado al Maximum Independent Set (MIS). Un conjunto de vértices \(V'\) es una cubierta de vértices si y solo si su complemento \(V \setminus V'\) es un conjunto independiente. Por lo tanto, minimizar \(|V'|\) es equivalente a maximizar \(|V \setminus V'|\), es decir, maximizar el tamaño del Conjunto Independiente. La dificultad de aproximar MIS se traslada directamente a la dificultad de aproximar MVC.

La reducción \(\mathcal{R}\) transforma una instancia de MVC \(\langle G \rangle\) en una instancia de MCCPP \(\langle G', F, C \rangle\):

\begin{itemize}

\item \textbf{Grafo \(G'\)}: Se mantiene el grafo original, \(G' = G = (V, E)\).

\item \textbf{Conjunto de Etiquetas \(F\)}: Se define un conjunto de etiquetas \(F = \{f_1, f_2, \ldots, f_n\}\), donde \(n = |V|\). Este conjunto es suficiente para cualquier coloración.

\item \textbf{Matriz de Costos \(C\)}: Se definen costos binarios de la siguiente manera:

\begin{itemize}

\item \textbf{Costo Cero (Color Preferido)}: \(c_{v, f_1} = 0\), para todo vértice \(v \in V\).

\item \textbf{Costo Unitario (Otros Colores)}: \(c_{v, f} = 1\), para todo \(v \in V\) y \(f \in \{f_2, \ldots, f_n\}\).

\end{itemize}

\end{itemize}

El objetivo del MCCPP en esta instancia es encontrar una coloración propia \(\phi: V \to F\) que minimice el costo total \(Z_{MCCPP} = \sum_{v \in V} c_{v, \phi(v)}\).

\textbf{Equivalencia de la Optimalidad:}

Para minimizar \(Z_{MCCPP}\), se debe maximizar el número de vértices asignados al color \(f_1\) (costo 0). Sea \(I_1\) el conjunto de vértices coloreados con \(f_1\). Para que la coloración sea válida, \(I_1\) debe ser un conjunto independiente.

El costo óptimo \(Z_{MCCPP}^*\) se logra cuando \(I_1\) es el Máximo Conjunto Independiente (\(I_{\max}\)), ya que todos los demás colores (\(f_2, \ldots, f_n\)) tienen costo 1.

\[
Z_{MCCPP}^* = 0 \cdot |I_{\max}| + 1 \cdot (|V| - |I_{\max}|) = |V| - |I_{\max}|
\]

Dado que el tamaño del Minimum Vertex Cover \(Z_{MVC}^*\) es igual a \(|V| - |I_{\max}|\), se establece la relación:

\[
Z_{MCCPP}^* = Z_{MVC}^*
\]

\textbf{Preservación de la Aproximación:}

Si existiera un algoritmo de aproximación \(A_{MCCPP}\) para el MCCPP con factor de aproximación constante \(\alpha\), es decir, \(Z_{MCCPP}^{Alg} \le \alpha \cdot Z_{MCCPP}^*\), este algoritmo podría usarse para resolver MVC con el mismo factor:

\[
Z_{MVC}^{Alg} = Z_{MCCPP}^{Alg} \le \alpha \cdot Z_{MCCPP}^* = \alpha \cdot Z_{MVC}^*
\]

Dado que el MVC es APX-Hard, la existencia de una L-Reducción con preservación lineal del valor óptimo demuestra que el MCCPP es, a su vez, APX-Hard.

\subsubsection{Implicaciones Teóricas: Exclusión del PTAS y APX-Completeness}

La demostración de que el MCCPP es APX-Hard tiene consecuencias directas sobre la existencia de esquemas de aproximación.

\subsubsection{Imposibilidad de PTAS}

La APX-Hardness establece que, si \(P \neq NP\), el MCCPP no admite un PTAS. Esta limitación persiste incluso en el caso especial de costos binarios, tal como se demostró en la reducción a MVC.

\subsubsection{Análisis Riguroso de Pertenencia a APX (MCCPP)}

Un problema es APX-Complete si es APX-Hard y también pertenece a la clase APX (es decir, admite un algoritmo de aproximación de factor constante \(c\)).

Aunque el MCCPP es APX-Hard, su pertenencia a APX no está universalmente establecida para grafos generales.

Como la pertenencia de un problema APX-Hard a la clase APX requiere demostrar la existencia de un algoritmo de aproximación con un factor de rendimiento constante \(c \ge 1\) en tiempo polinomial, para el MCCPP esta demostración se considera altamente improbable para grafos generales.

\textbf{Modelado Formal como Cobertura de Conjuntos Ponderada (WSC)}

El MCCPP se puede modelar como el \textbf{Problema de Cobertura de Conjuntos Ponderada} (Weighted Set Cover, WSC), un problema fundamental de optimización ampliamente estudiado tanto desde el punto de vista algorítmico como de inaproximabilidad \cite{feige1998setcover,setcoverChronology2021}.

\begin{itemize}

\item \textbf{Definición Formal de WSC}: Dada una instancia \(\langle U, \mathcal{S}, w \rangle\), donde \(U\) es el universo de elementos a cubrir (\(|U|=n\)), \(\mathcal{S} = \{S_1, \ldots, S_m\}\) es una familia de subconjuntos de \(U\), y \(w: \mathcal{S} \to \mathbb{R}^+\) es la función de peso (costo) de cada subconjunto \(S_i\). El objetivo es encontrar una subcolección \(\mathcal{C} \subseteq \mathcal{S}\) tal que \(\bigcup_{S_i \in \mathcal{C}} S_i = U\), minimizando el costo total \(\sum_{S_i \in \mathcal{C}} w(S_i)\).

\end{itemize}

\textbf{Mapeo MCCPP a WSC:}

Una instancia \(\langle G, F, C \rangle\) del MCCPP se transforma en una instancia \(\langle U, \mathcal{S}, w \rangle\) del WSC:

\begin{enumerate}

\item \textbf{Universo (\(U\))}: El conjunto de vértices \(V\) del grafo \(G\). El objetivo es “cubrir” todos los vértices, es decir, asignarles un color \cite{feige1998setcover,setcoverChronology2021}.

\item \textbf{Conjuntos Candidatos (\(\mathcal{S}\))}: Cada conjunto \(S \in \mathcal{S}\) es un par \(\langle I, f \rangle\), donde \(I \subseteq V\) es un \textit{Conjunto Independiente} (IS) válido en \(G\) y \(f \in F\) es una frecuencia (color) disponible \cite{feige1998setcover,setcoverChronology2021}.

\item \textbf{Costo del Conjunto \(w(S)\)}: El costo de elegir el conjunto \(S = \langle I, f \rangle\) es el costo total de asignar la frecuencia \(f\) a todos los vértices en \(I\):
\[
w(S) = \sum_{v \in I} c_{v,f}
\]

\end{enumerate}

Una solución válida del WSC (una subcolección de conjuntos independientes que cubre \(V\)) corresponde directamente a una partición cromática válida de \(G\) con el mismo costo total.

\textbf{Demostración de Inaproximabilidad Constante (MCCPP \(\notin\) APX)}

El mejor algoritmo de aproximación conocido para el WSC es el algoritmo \textbf{Greedy}, que alcanza una cota asintótica óptima en términos del factor logarítmico \(\Theta(\ln n)\) tanto para Set Cover como para su versión ponderada \cite{feige1998setcover,setcoverChronology2021}.

Sea \(Z_{WSC}^*\) el costo óptimo del WSC y \(Z_{WSC}^{Alg}\) el costo de la solución obtenida por el algoritmo voraz.

\begin{itemize}

\item \textbf{Factor de Aproximación Logarítmico}: El algoritmo Greedy garantiza un factor de aproximación \(R\) que está acotado superiormente por el número armónico \(H(|U|)\), donde \(|U|=|V|=n\):
\[
R = \frac{Z_{WSC}^{Alg}}{Z_{WSC}^*} \le H(|U|) = O(\ln n)
\]

\item \textbf{Límite Inferior de Inaproximabilidad}: Se ha demostrado rigurosamente que el WSC (y por extensión el Set Cover estándar) no admite un algoritmo de aproximación de factor constante \(c\). Específicamente, a menos que \(NP \subseteq DTIME(n^{\log \log n})\), no existe un algoritmo que garantice un factor de aproximación mejor que \((1 - \epsilon)\ln n\), para cualquier \(\epsilon > 0\) \cite{feige1998setcover,setcoverChronology2021}.

\end{itemize}

Dado que la mejor aproximación garantizada para el MCCPP es logarítmica \(O(\ln |V|)\), y existe un límite inferior que excluye factores constantes, se concluye formalmente que el MCCPP \textbf{no pertenece} a la clase APX, y por lo tanto, no es APX-Complete. En términos de la jerarquía de aproximación, el MCCPP se clasifica en la clase \(\mathbf{log}\)-APX.

\subsection{Análisis de Complejidad Parametrizada}

La \textbf{Complejidad Parametrizada} ofrece un marco teórico para analizar problemas NP-Hard en función de parámetros específicos que pueden influir en la complejidad computacional. En este contexto, se examina la dificultad del MCCPP cuando se parametriza por el número de frecuencias \(k = |F|\), por el treewidth \(tw\) del grafo \(G\) y otros enfoques.

\textbf{FPT (Fixed-Parameter Tractable)}: Un problema parametrizado \(\langle I, k \rangle\) es FPT si existe un algoritmo que resuelve cualquier instancia en tiempo \(O(f(k) \cdot |I|^c)\), donde \(f\) es una función computable que depende únicamente del parámetro \(k\), y \(c\) es una constante independiente de \(k\) y \(|I|\). Esto implica que la complejidad exponencial está confinada al parámetro \(k\), permitiendo soluciones eficientes para valores pequeños de \(k\).

\subsubsection{Definición Formal de W-Hardness}

La \textbf{Complejidad Parametrizada} clasifica los problemas NP-Hard basándose en cómo el parámetro \(k\) afecta la complejidad exponencial. Un problema \(\langle I, k \rangle\) se clasifica en la jerarquía \(\mathbf{W}\) si no es FPT (asumiendo que \(\mathbf{FPT} \neq \mathbf{W}\)).

La clase \(\mathbf{W}[\mathbf{1}]\) se define mediante problemas que son equivalentes bajo FPT-reducciones al problema \textit{Weighted Weft-1-Depth-\(d\) SAT} o, más comúnmente, al problema canónico \(\mathbf{k}\)-Clique.

\textbf{Definición de W-Hardness:} Un problema parametrizado \(P\) es \(\mathbf{W}[\mathbf{1}]\)-Hard si todo problema en \(\mathbf{W}[\mathbf{1}]\) puede ser reducido a \(P\) mediante una \textbf{FPT-reducción}. Una FPT-reducción es una transformación de una instancia \(\langle I, k \rangle\) de \(P_1\) a una instancia \(\langle I', k' \rangle\) de \(P_2\), computable en tiempo \(O(f(k) \cdot |I|^c)\), donde el nuevo parámetro \(k'\) depende únicamente de \(k\) (\(k' \le g(k)\)).

La consecuencia fundamental de la \(\mathbf{W}[\mathbf{1}]\)-Hardness es que, si se cree que la hipótesis de \(\mathbf{FPT} \neq \mathbf{W}[\mathbf{1}]\) es verdadera, el problema no admite un algoritmo de tiempo \(O(f(k) \cdot n^c)\), lo que descarta la kernelización con un tamaño que dependa únicamente de \(k\).

\subsubsection*{Problema \(k\)-Coloring es W-Hard}

La dificultad de la coloración se traslada directamente al reino de la complejidad parametrizada. Entonces se tiene que el problema \(\mathbf{k}\)-Coloring, parametrizado por el número de colores \(k\), es \(\mathbf{W}[\mathbf{1}]\)-Hard, tal como se establece en la literatura clásica de complejidad parametrizada y resultados específicos sobre problemas coloridos parametrizados por treewidth \cite{flum2006colorful}.

\subsubsection*{Fundamentación Académica}

Este resultado, que establece que la coloración es paramétricamente difícil, se basa en reducciones desde problemas centrales de la jerarquía \(W\), como \(k\)-Clique o su variante Multicolored Clique. La demostración completa se encuentra rigurosamente detallada en trabajos de Flum y Grohe sobre problemas coloridos parametrizados por treewidth y en manuales estándar de complejidad parametrizada \cite{flum2006colorful,cygan2015parameterized}.

\subsubsection{Demostración de que el MCCPP es \(\mathbf{W}[\mathbf{1}]\)-Hard cuando se parametriza por \(|F|\)}

El resultado de la \(W\)-Hardness del \(k\)-Coloring tiene implicaciones directas para el MCCPP, ya que la restricción de factibilidad del MCCPP es idéntica a la del \(k\)-Coloring.

\subsubsection*{Planteamiento}

El Problema de Partición Cromática de Costo Mínimo (MCCPP), parametrizado por el número de frecuencias \(k=|F|\), es \(\mathbf{W}[\mathbf{1}]\)-Hard.

\subsubsection*{Demostración}

La demostración se logra mediante la \textbf{FPT-Reducción trivial} del \(k\)-Coloring al MCCPP:

\begin{enumerate}

\item Se parte de una instancia del \(k\)-Coloring \(\langle G, k \rangle\).

\item Se construye la instancia del MCCPP \(\langle G, F, C \rangle\) con \(|F|=k\), estableciendo todos los costos \(c_{v, f} = 0\).

\item El tiempo de esta construcción es \(\mathbf{O(|V| \cdot k)}\), que satisface la condición FPT (polinomial en \(|V|\), pero la dependencia exponencial es solo en \(k\)).

\item El parámetro se preserva: \(k' = k\).

\end{enumerate}

Dado que la existencia de una solución de costo cero en el MCCPP es equivalente a la existencia de una \(k\)-coloración válida, y la reducción es una FPT-reducción, se concluye que \(\mathbf{MCCPP}\) es \(\mathbf{W}[\mathbf{1}]\)-Hard cuando se parametriza por el número de colores \(k\).

\subsubsection{Demostración de que el MCCPP es \(\mathbf{W}[\mathbf{1}]\)-Hard cuando se parametriza por Treewidth}

\textbf{Estado de la Cuestión:}

Existe literatura más antigua (< 2000) donde se afirma que el MCCPP es FPT por \(tw\). Sin embargo, en investigaciones más recientes (2017) se refuta lo afirmado en la literatura inicial sobre el problema: se ha demostrado formalmente mediante una reducción parametrizada rigurosa que el Problema de Partición Cromática de Costo Mínimo (MCCPP), también conocido como Weighted Coloring, es \textbf{W-hard} cuando se parametriza por la \textbf{treewidth} del grafo, incluso en el caso restringido de bosques (donde la treewidth es exactamente 1) \cite{araujo2017weightedforests}.

\subsubsection*{Equivalencia Formal entre MCCPP y Weighted Coloring}

Replanteemos el problema del MCCPP y el Weighted Coloring para establecer su equivalencia formal.

\textbf{Problema 1: Minimum Cost Chromatic Partition Problem (MCCPP):}

El Problema de Partición Cromática de Costo Mínimo se define formalmente de la siguiente manera:

\textbf{Instancia:} Un grafo no dirigido \(G = (V, E)\) y una matriz de costos \(C\) donde \(c_{v, f}\) es el costo de asignar la etiqueta (frecuencia/color) \(f\) al vértice \(v\). Un conjunto finito \(F\) de etiquetas disponibles con \(|F| = k\).

\textbf{Objetivo:} Encontrar una función de asignación \(\phi: V \to F\) (una coloración propia, es decir, \(\phi(u) \neq \phi(v)\) para toda arista \(\{u,v\} \in E\)) que minimice el costo total:
\[
Z_{MCCPP} = \min_{\phi \text{ coloración propia}} \sum_{v \in V} c_{v, \phi(v)}
\]

\textbf{Problema 2: Weighted Coloring (Weighted Chromatic Number):}

El problema de Weighted Coloring, introducido formalmente por Guan y Zhu, se define como sigue \cite{guan1997weightedcoloring}:

\textbf{Instancia:} Un grafo no dirigido \(G = (V, E)\) y una función de pesos \(w: V \to \mathbb{R}^+\) que asigna un peso positivo a cada vértice.

\textbf{Definiciones de Colores y Pesos:}

Una coloración propia \(c\) de \(G\) es una partición \(c = (S_i)_{i \in [1, k]}\) del conjunto \(V\) en \(k\) conjuntos independientes (también llamados estables) \(S_1, S_2, \ldots, S_k\).

Dado que cada conjunto \(S_i\) es un conjunto independiente de vértices del mismo color, el \textit{peso} del color \(S_i\) se define como:

\[
w(i) = \max_{v \in S_i} w(v)
\]

es decir, el peso máximo de los vértices en ese color.

El \textit{peso total} de una coloración \(c\) es:

\[
w(c) = \sum_{i=1}^{k} w(i) = \sum_{i=1}^{k} \max_{v \in S_i} w(v)
\]

\textbf{Objetivo:} Calcular la \textit{weighted chromatic number} \(\sigma(G, w)\), definida como el peso mínimo entre todas las coloraciones propias válidas de \(G\):

\[
\sigma(G, w) = \min_{c \text{ coloración propia}} w(c)
\]

Una coloración \(c\) tal que \(w(c) = \sigma(G, w)\) se llama coloración óptima ponderada.

\subsubsection*{Relación entre MCCPP y Weighted Coloring}

El Weighted Coloring es un caso especial del MCCPP, no son exactamente equivalentes. Es importante establecer esta relación con precisión:

\textbf{Weighted Coloring:}
\begin{itemize}
	\item Cada vértice \(v\) tiene un peso fijo \(w(v) \in \mathbb{R}^+\)
	\item El costo de un color \(i\) es \(w(i) = \max_{v \in S_i} w(v)\)
	\item Objetivo: minimizar \(\sum_{i=1}^{k} w(i) = \sum_{i=1}^{k} \max_{v \in S_i} w(v)\)
\end{itemize}

\textbf{MCCPP:}
\begin{itemize}
	\item Cada par (vértice \(v\), frecuencia \(f\)) tiene un costo independiente \(c_{v,f}\)
	\item El costo total es \(\sum_{v \in V} c_{v,\phi(v)}\)
	\item Más general: permite costos arbitrarios por par (torre, frecuencia)
\end{itemize}

\textbf{Relación formal:}
Weighted Coloring \(\subseteq\) MCCPP. Se puede modelar Weighted Coloring como un caso especial del MCCPP estableciendo \(c_{v,f} = w(v)\) para todo \(f\), de modo que el costo de cada vértice es independiente del color asignado. En este caso especial, minimizar \(\sum_{v} c_{v,\phi(v)}\) bajo la estructura de conjuntos independientes reproduce el problema de Weighted Coloring.

Sin embargo, el MCCPP permite estructuras de costo más complejas donde la misma torre puede tener costos muy diferentes según la frecuencia asignada, reflejando factores como:
\begin{itemize}
	\item Eficiencia energética variable según la banda de frecuencia
	\item Costos de licencias específicos por frecuencia y ubicación
	\item Compatibilidad del equipamiento existente con ciertas frecuencias
\end{itemize}

\textbf{Implicación para W[1]-hardness:}

Los resultados de W[1]-hardness de Weighted Coloring \cite{araujo2017weightedforests} se transfieren al MCCPP porque:
\begin{enumerate}
	\item Si un caso especial (Weighted Coloring) es W[1]-hard
	\item Entonces el problema general (MCCPP) también es W[1]-hard
	\item La reducción se hereda: cualquier instancia de Weighted Coloring puede expresarse como instancia del MCCPP
\end{enumerate}

Por lo tanto, las demostraciones de W-hardness para Weighted Coloring parametrizado por treewidth o número de colores establecen automáticamente que el MCCPP más general también es W-hard bajo esos mismos parámetros.

\textbf{Referencia Académica:}

Esta relación entre ambos problemas está implícita en los trabajos de Guan y Zhu sobre coloración ponderada \cite{guan1997weightedcoloring}, y los resultados de complejidad parametrizada se estudian específicamente para Weighted Coloring en \cite{araujo2017weightedforests,baste2018dualparameterization}, aplicándose al MCCPP por contención.

\subsubsection*{Independent Set es W-Completo}

Antes de proceder a la demostración de \(\mathbf{W}[\mathbf{1}]\)-hardness del MCCPP mediante reducción desde Independent Set, es esencial establecer que Independent Set mismo es un problema canónico W-completo en complejidad parametrizada.

\subsubsection*{Definición del Problema Independent Set}

\textbf{Instancia:} Un grafo no dirigido \(G = (V, E)\) y un entero positivo \(k\) (parámetro).

\textbf{Pregunta:} ¿Existe un conjunto \(S \subseteq V\) de vértices con \(|S| \geq k\) tal que ningún par de vértices en \(S\) es adyacente? Es decir, ¿existe un conjunto independiente de tamaño al menos \(k\)?

\subsubsection*{Planteamiento: Independent Set es W-Completo}

El problema Independent Set parametrizado por el tamaño de la solución \(k\) es \textbf{W-completo}. Esta es una de las conclusiones fundamentales de la teoría de complejidad parametrizada, establecida desde los trabajos fundacionales de Downey y Fellows y sistematizada en manuales modernos \cite{downey2013fundamentals,cygan2015parameterized}.

\subsubsection*{Implicaciones}

La W-completeness de Independent Set significa que dos cosas son ciertas:

\begin{enumerate}

\item \textbf{Independent Set pertenece a W:} Existe un certificador eficiente (en términos de complejidad parametrizada) que puede verificar en tiempo \(f(k) \cdot n^c\) si un conjunto \(S\) de tamaño \(k\) es un conjunto independiente válido. Esta verificación es trivial: simplemente revisar si existen aristas entre los vértices de \(S\), lo cual toma \(O(k^2)\) tiempo.

\item \textbf{Todo problema en W puede ser reducido a Independent Set mediante una FPT-reducción:} La demostración se basa en mostrar que la definición de W mediante máquinas de Turing ponderadas puede ser capturada por Independent Set. Específicamente, cualquier problema definible como “¿existe una solución de tamaño exactamente \(k\) que satisfaga la propiedad \(P(x, k)\)?” (donde \(P\) es verificable en tiempo FPT) puede ser reducido a Independent Set.

\end{enumerate}

\subsubsection*{Referencia Académica}

Estos resultados fundamentales están exhaustivamente cubiertos en \cite{cygan2015parameterized,downey2013fundamentals}, donde se expone la jerarquía W y se demuestra rigurosamente que Independent Set parametrizado por \(k\) es W-completo.

\subsubsection*{Demostración de \(\mathbf{W}[\mathbf{1}]\)-Hardness del MCCPP vía Weighted Coloring}

\subsubsection*{Teorema Principal}

El Problema de Partición Cromática de Costo Mínimo (MCCPP), cuando se parametriza por la \textbf{treewidth} del grafo (o más generalmente, por el tamaño del mayor componente conexo del grafo), es \textbf{W-hard}.

\textbf{Demostración de \(\mathbf{W}[\mathbf{1}]\)-Hardness por Reducción desde Independent Set}

La demostración se logra mediante una \textbf{FPT-reducción parametrizada} desde el problema canónico \(\mathbf{W}[\mathbf{1}]\)-hard de \textit{Independent Set} hacia Weighted Coloring y, por equivalencia, hacia MCCPP \cite{araujo2017weightedforests}.

\textbf{Definición de Problemas:}

\begin{itemize}

\item \textbf{Independent Set (IS):} Dado un grafo \(G=(V,E)\) y un entero \(k\), ¿existe un conjunto \(S \subseteq V\) con \(|S| \geq k\) tal que ningún par de vértices en \(S\) es adyacente?

\item \textbf{Weighted Coloring (WC):} Dado un grafo ponderado \((G, w)\) con función de pesos \(w: V \to \mathbb{R}^+\), donde el peso de un color \(S_i\) se define como \(w(i) = \max_{v \in S_i} w(v)\), ¿existe una coloración propia \(c = (S_i)_{i=1}^k\) tal que \(w(c) = \sum_{i=1}^k w(i) \leq M\) para un umbral \(M\)?

\end{itemize}

\textbf{Construcción de la Reducción:}

La reducción construye a partir de una instancia \((G, k)\) de Independent Set una instancia ponderada \((G', w)\) de Weighted Coloring, introduciendo gadgets especializados:

\begin{enumerate}

\item \textbf{Árboles Binomiales Ponderados \(B_i\)}: Para cada \(i \in [0, 4k+3]\), se definen recursivamente árboles binomiales con pesos específicos \(w_i^0 = 1/2^i + j\varepsilon\), donde \(\varepsilon\) es un parámetro pequeño seleccionado cuidadosamente. Estos árboles fuerzan que en cualquier coloración de peso \(\leq M\), todos los vértices de un árbol binomial reciban el mismo color de forma determinista.

\item \textbf{Árboles Auxiliares \(A_i^j\)}: Para cada índice \(i \in [0, 4k-1]\) y \(j \in [0, n]\), se construyen árboles auxiliares que contienen copias de árboles binomiales. Su función es actuar como verificadores de que los vértices originales se han seleccionado de forma consistente a lo largo de diferentes copias.

\item \textbf{Gadget AND (\(R_0\)-AND)}: Un circuito lógico ponderado que implementa la operación lógica AND entre dos entradas, garantizando que si ambas entradas reciben un color específico \(R_0\), la salida debe también recibir \(R_0\).

\item \textbf{Árboles de Vértices \(T_i^j\)}: Para cada \((i, j) \in [0, k-1] \times [0, n-1]\), se construye un árbol que codifica si el vértice \(\beta^{-1}(j)\) (donde \(\beta\) es una biyección \(V \to [0, n-1]\)) pertenece o no al conjunto independiente.

\end{enumerate}

\textbf{Parámetro Umbral \(M\):}

Se define \(M = k(n-1)\varepsilon + \sum_{i=0}^{4k+3} \frac{1}{2^i}\) con \(0 < \varepsilon < \frac{1}{nk2^{4k+3}}\).

Esta selección garantiza que:

\begin{itemize}

\item \(M < 2\), proporcionando un umbral bien definido.

\item Cada vez que se selecciona un vértice para el conjunto independiente (coloreando su raíz con \(R_0\)), se incurre en un costo adicional de \((n-1)\varepsilon\).

\item Solo es posible seleccionar exactamente \(k\) vértices sin exceder el presupuesto \(M\).

\end{itemize}

\textbf{Lema de Correspondencia:}

Sea \((T, w)\) un bosque ponderado que contiene, para cada \((i,j) \in [0, k-1] \times [0, n-1]\), la estructura \(T_i^j\) como subárbol. Sea \(c\) una coloración de \((T, w)\) con \(w(c) \leq M\). Entonces existen índices \((j_i)_{i \in [0, k-1]} \in [0, n-1]^k\) tales que para cada raíz \(u\) de \(T_{j}^{i}\):

\begin{itemize}

\item Si \(j = j_i\) para algún \(i \in [0, k-1]\), entonces \(c(u) = R_0\).

\item Si \(j \neq j_i\) para todo \(i \in [0, k-1]\), entonces \(c(u) = R_1\).

\end{itemize}

\textbf{Prueba de Equivalencia:}

La FPT-reducción satisface: existe un conjunto independiente de tamaño exactamente \(k\) en \(G\) si y solo si existe una coloración \((G', w)\) con \(w(c) \leq M\).

\begin{itemize}

\item \textbf{(Adelante, \(\Rightarrow\))}: Si \(Z\) es un conjunto independiente de tamaño \(k\) en \(G\), se puede construir una coloración válida asignando cada vértice \(v \in Z\) al color \(R_0\) (costo 0 en las estructuras correspondientes) en los árboles \(T_i^j\) donde \(j = \beta(v)\). Para aristas \(\{v_1, v_2\} \in E\) con \(v_1 \in Z, v_2 \notin Z\), los gadgets AND se pueden satisfacer con el color \(R_1\) en la salida, manteniendo \(w(c) = M\).

\item \textbf{(Atrás, \(\Leftarrow\))}: Si existe una coloración con \(w(c) \leq M\), por el Lema anterior, existen índices \((j_i)\) tales que exactamente \(k\) vértices reciben color \(R_0\). Definiendo \(Z = \{\beta^{-1}(j_i) : i \in [0, k-1]\}\), se verifica que no hay arista entre elementos de \(Z\). Si existiera una arista \(\{v_1, v_2\}\) con \(v_1, v_2 \in Z\), entonces los gadgets AND en \(H_{\{v_1, v_2\}, i_1, i_2}\) forzarían que su raíz se coloreara con \(R_0\), lo que contradice el árbol binomial \(B_{4k}\) conectado a ella.

\end{itemize}

\textbf{Complejidad de la Reducción:}

La construcción requiere:

\begin{itemize}

\item Tiempo: \(f(k) \cdot n^{O(1)}\) donde \(f(k) = 2^{O(k)}\) es una función computable de \(k\).

\item El parámetro se preserva: \(k' = k\) (el parámetro de la nueva instancia depende solo del parámetro de la instancia original).

\item El tamaño de cada componente conexo de \((G', w)\) está acotado por \(O(k)\).

\end{itemize}

\subsubsection*{Conclusión}

El Problema de Partición Cromática de Costo Mínimo (MCCPP), cuando se parametriza por la \textbf{treewidth} del grafo (o por el tamaño del mayor componente conexo), es \textbf{W-hard}. Por lo tanto, a menos que \(\mathbf{FPT} = \mathbf{W[1]}\), \textbf{no existe un algoritmo FPT para el MCCPP parametrizado por treewidth}. Este resultado es válido incluso en la clase altamente restringida de bosques (treewidth = 1), lo que hace aún más fuerte la imposibilidad \cite{araujo2017weightedforests}.[1]

\textbf{Corolario sobre la Optimización Observada en Bounded Treewidth:}

Guan y Zhu observaron que en grafos de treewidth acotada \(t\), se podía resolver \(\sigma(G, w; r)\) en tiempo \(n^{O(r)} \cdot r^{O(t)}\) mediante programación dinámica estándar. Sin embargo, este tiempo no es FPT porque la dependencia en \(n\) es polinomial de grado potencialmente alto, y más crucialmente, porque el parámetro \(r\) (número de colores) está acoplado de forma multiplicativa con el tamaño de la entrada. La demostración de W-hardness implica que no puede haber mejora fundamental en esta complejidad (bajo conjeturas estándar de complejidad parametrizada) \cite{guan1997weightedcoloring,araujo2017weightedforests}.[1]

\subsubsection*{Referencia Académica}

Los detalles completos de la reducción y de los gadgets utilizados para demostrar la W-hardness de Weighted Coloring (y, por equivalencia, de MCCPP) en bosques pueden encontrarse en \cite{araujo2017weightedforests}.

\subsubsection{Búsqueda de Parámetros Alternativos FPT para el MCCPP}

Dado que se ha demostrado que el MCCPP es W-hard con respecto a las frecuencias \(f\) (equivalente a número de colores \(r\)) y treewidth, es natural investigar si existen otros parámetros para los cuales el problema sea FPT.

\textbf{Panorama de Parámetros Estudiados:}

Los siguientes parámetros han sido analizados en la literatura sobre problemas de coloración ponderada y problemas relacionados:

\begin{enumerate}

\item \textbf{Vertex Cover:} Este parámetro mide la mínima cantidad de vértices cuya eliminación convierte al grafo en una unión disjunta de conjuntos independientes. Aunque para coloración simple (\(k\)-Coloring) existen algoritmos FPT parametrizados por vertex cover, para MCCPP la literatura académica no reporta resultados positivos.

\item \textbf{Modular Decomposition Width:} Más restrictivo que treewidth, pero tampoco se ha demostrado FPT para MCCPP.

\item \textbf{Clique-Width:} Para coloración estándar se conocen limitaciones, pero MCCPP con costos no ha sido completamente caracterizado.

\item \textbf{Pathwidth:} Similar a treewidth pero con separadores lineales en lugar de árboles. Por la reducción desde Independent Set, también es W-hard para MCCPP.[1]

\item \textbf{Número de Colores \(r\) (Restricción):} Se puede demostrar que MCCPP es W-hard cuando se parametriza por \(r\) (el número de colores permitidos) \cite{araujo2017weightedforests}.[2]

\end{enumerate}

\textbf{Resultado de W-Hardness para el Parámetro \(r\):}[2]

Siguiendo técnicas de reducción desde Dominating Set y resultados sobre Weighted Coloring, se obtiene:

\subsubsection*{Planteamiento}

El problema de determinar \(\sigma(G, w; r)\) (el costo mínimo de una coloración propia usando exactamente \(r\) colores) en un grafo ponderado es \textbf{W-hard} cuando se parametriza únicamente por \(r\).[2]

Esto implica que incluso si se fija el número de colores a utilizar, el problema sigue siendo paramétricamente intratable (a menos que \(\mathbf{W[2]} = \mathbf{FPT}\), lo que implicaría un colapso de la jerarquía W) \cite{araujo2017weightedforests}.

\textbf{Análisis de Falta de FPT para Parámetros Naturales:}

Por la reducción desde Independent Set y por los resultados de W-hardness para Weighted Coloring, los siguientes parámetros naturales \textbf{también son W-hard} para MCCPP, debido a que están todos acotados por la talla del mayor componente conexo:[1]

\begin{itemize}

\item Treewidth del grafo

\item Pathwidth del grafo

\item Degeneracy del grafo

\item Número cromático \(\chi(G)\)

\item Feedback Vertex Set (FVS)

\item Distance to Clique

\item Distance to Coloring

\item Tree-Depth del grafo

\item Clique-Cover Number

\end{itemize}

\subsubsection*{Conclusión}

Basado en los resultados de Araújo, Baste y Sau, el \textbf{Problema de Partición Cromática de Costo Mínimo (MCCPP) no es FPT para ningún parámetro de estructura de grafo \textit{estándar}} (treewidth, pathwidth, treedecomposición, vertex cover, etc.) \cite{araujo2017weightedforests}.

Los únicos parámetros que potencialmente podrían hacer MCCPP FPT serían:

\begin{enumerate}

\item \textbf{Parámetros combinados muy específicos:} Por ejemplo, una combinación del número de colores \(r\) y una cota en el número de aristas o una cota fuerte en \(k\) (el tamaño del conjunto independiente máximo).

\item \textbf{Parámetros de distancia a clases especiales:} Distancia a grafos completos, distancia a árboles muy pequeños, o similares. Pero estos son ampliamente restrictivos y no capturan muchos grafos de interés práctico.

\item \textbf{Parametrización por la solución:} Parametrizar por el valor óptimo \(\sigma(G, w)\) mismo, lo cual es una parametrización “circular” y rara vez se considera en análisis de complejidad parametrizada.

\end{enumerate}

\section{Diseño de Soluciones Algorítmicas}

El MCCPP exige dividir \(V\) en conjuntos independientes minimizando un costo asignado a cada clase de color. Su dificultad combinatoria, dado que MCCPP es NP-Hard y APX-Hard, requiere un enfoque múltiple: exacto, aproximado, metaheurístico y estructural.

\subsection{Soluciones Exactas (Bases de Referencia)}

\subsubsection{Algoritmo de Fuerza Bruta: Enumeración Completa}

El algoritmo de fuerza bruta es esencialmente un algoritmo de Branch-and-Bound sin poda, cuyo propósito es encontrar la solución óptima \(Z^*\) en instancias de tamaño muy pequeño para servir como línea base experimental.

\textbf{Mecanismo:} El algoritmo explora sistemáticamente todas las posibles asignaciones de frecuencias a vértices. Para una instancia \(\langle G, F, C \rangle\), con \(|V|=n\) y \(|F|=k\), se genera cada función de mapeo \(\phi: V \to F\). Para cada asignación, se verifica la restricción de coloración propia, y si es válida, se calcula el costo total \(\sum_{v \in V} c_{v, \phi(v)}\). 

\paragraph{Pseudocódigo Formal\\}

\begin{algorithm}[H]
	\caption{Fuerza Bruta para MCCPP}
	\KwIn{Grafo $G = (V, E)$, $|V| = n$, $|E| = m$; Conjunto de colores $F$, $|F| = k$; Matriz de costos $C[v, f]$ para $v \in V, f \in F$}
	\KwOut{Coloración óptima $\varphi^*: V \to F$; Costo óptimo $Z^*$}
	
	$Z^* \leftarrow +\infty$\;
	$\varphi^* \leftarrow \texttt{NULL}$\;
	
	\ForEach{función $\varphi: V \to F$}{
		$\textit{factible} \leftarrow \texttt{TRUE}$\;
		
		\ForEach{arista $\{u, v\} \in E$}{
			\If{$\varphi(u) = \varphi(v)$}{
				$\textit{factible} \leftarrow \texttt{FALSE}$\;
				\textbf{break}\;
			}
		}
		
		\If{$\textit{factible}$}{
			$Z_{\text{actual}} \leftarrow \sum_{v \in V} C[v, \varphi(v)]$\;
			
			\If{$Z_{\text{actual}} < Z^*$}{
				$Z^* \leftarrow Z_{\text{actual}}$\;
				$\varphi^* \leftarrow \varphi$\;
			}
		}
	}
	
	\Return{$(\varphi^*, Z^*)$}
\end{algorithm}

\paragraph{Demostración de Correctitud:}

El algoritmo de Fuerza Bruta encuentra la solución óptima $Z^*$ para cualquier instancia $\langle G, F, C \rangle$ del MCCPP. Demostramos por tres propiedades fundamentales:

\textbf{(1) Completitud (explora todo el espacio):}
\begin{itemize}
	\item El algoritmo enumera todas las $k^n$ funciones posibles $\varphi: V \to F$ (línea 3).
	\item Por construcción, cualquier solución factible $\varphi_{\text{opt}}$ es una de estas $k^n$ configuraciones.
	\item Por tanto, $\varphi_{\text{opt}}$ será examinada en alguna iteración $i$.
\end{itemize}

\textbf{(2) Verificación de factibilidad:}
\begin{itemize}
	\item Las líneas 5--8 verifican la restricción de coloración propia: $\forall \{u,v\} \in E, \varphi(u) \neq \varphi(v)$.
	\item Esta verificación es exacta: una configuración pasa la prueba si y solo si es una coloración propia válida.
\end{itemize}

\textbf{(3) Optimalidad:}
\begin{itemize}
	\item Sea $Z_{\text{opt}}$ el costo de la solución óptima.
	\item En la iteración $i$ donde se examina $\varphi_{\text{opt}}$:
	\begin{itemize}
		\item $\textit{factible} = \texttt{TRUE}$ (es válida)
		\item $Z_{\text{actual}} = \sum_{v \in V} C[v, \varphi_{\text{opt}}(v)] = Z_{\text{opt}}$ (línea 10)
		\item Como $Z_{\text{opt}} \leq Z^*$ (por ser óptima), la condición de línea 11 se cumplirá en algún punto.
		\item $Z^*$ se actualiza a $Z_{\text{opt}}$ (línea 12).
	\end{itemize}
	\item Ninguna iteración posterior puede encontrar $Z_{\text{actual}} < Z_{\text{opt}}$ (por definición de óptimo).
	\item Por tanto, al terminar: $Z^* = Z_{\text{opt}}$. \qedhere
\end{itemize}

\paragraph{Análisis de Complejidad Temporal:} La complejidad temporal del algoritmo de Fuerza Bruta para MCCPP es $\Theta(k^n \cdot m)$.

Analizamos cada componente del algoritmo:

\begin{itemize}
	\item \textbf{Línea 3:} Genera $k^n$ configuraciones $\Rightarrow O(k^n)$ iteraciones.
	\item \textbf{Líneas 5--8:} Verifica $m$ aristas $\Rightarrow O(m)$ por iteración.
	\item \textbf{Línea 10:} Suma $n$ costos $\Rightarrow O(n)$ por iteración.
	\item \textbf{Complejidad total:} $k^n \cdot (m + n) = k^n \cdot O(m) = \Theta(k^n \cdot m)$ para $m \geq n$.
\end{itemize}

\textbf{Cota inferior ($\Omega$):}
\begin{itemize}
	\item Cualquier algoritmo que garantice optimalidad debe examinar al menos una solución óptima.
	\item En el peor caso (costos aleatorios), no hay forma de podar sin examinar todas las $k^n$ configuraciones.
	\item Por tanto: $\Omega(k^n \cdot m)$. \qedhere
\end{itemize}

Esta dependencia exponencial confirma que el algoritmo solo es viable para valores de \(n\) y \(k\) extremadamente pequeños.


\subsubsection{Backtracking y Backtracking inteligente:}

\textbf{Backtracking clásico}

El algoritmo de backtracking construye incrementalmente una coloración parcial del grafo, asignando colores a los vértices siguiendo un orden fijo 
$(v_1,\dots,v_n)$.  
En cada nivel de la recursión se asigna un color a un vértice siempre que dicha asignación no viole las restricciones de adyacencia con los vértices ya coloreados.

El algoritmo explora recursivamente el árbol de búsqueda hasta completar una asignación válida para todos los vértices, evaluando entonces su costo total.

\paragraph{Correctitud}

El algoritmo de backtracking devuelve una coloración óptima si se explora completamente el árbol de búsqueda.

\textit{Demostración:}  
Cada nodo del árbol de búsqueda corresponde a una coloración parcial factible.  
El algoritmo únicamente descarta ramas que violan la condición de coloración propia, por lo que toda coloración factible completa aparece como una hoja del árbol.

Dado que el algoritmo examina todas las coloraciones factibles, evalúa su costo y conserva la de menor valor, la solución devuelta es óptima por definición.

\paragraph{Complejidad temporal:}

En el peor caso, el algoritmo explora todas las posibles asignaciones de $k$ colores a $n$ vértices, lo que da lugar a una complejidad temporal de:

\[
O(k^n).
\]

La verificación local de factibilidad puede realizarse en tiempo proporcional al grado del vértice, lo cual queda absorbido por la cota exponencial dominante.

\medskip
\noindent
\textbf{Conclusión:} se trata de un algoritmo exacto, pero intratable para instancias de tamaño medio o grande.

\textbf{Backtracking inteligente con poda:\\}

La versión inteligente del backtracking introduce dos mejoras principales:

\begin{itemize}
	\item Ordenación heurística de los vértices, por ejemplo, por grado decreciente.
	\item Poda por costo (\textit{branch and bound}), descartando ramas cuyo costo parcial supera la mejor solución conocida.
\end{itemize}

Durante la exploración, el algoritmo mantiene el costo acumulado de la coloración parcial y evita explorar extensiones que no puedan conducir a una solución óptima.

\paragraph{Correctitud}

La poda por costo no elimina ninguna solución óptima.

\textit{Demostración:}  
Sea $C^*$ el costo de una solución óptima global y $C_{best}$ el menor costo encontrado hasta el momento.  
Una rama se poda únicamente cuando su costo parcial $C_p$ satisface $C_p \geq C_{best}$, con $C_{best} \leq C^*$.

Dado que cualquier extensión de dicha rama tendrá un costo total al menos $C_p$, ninguna solución descendiente puede mejorar $C_{best}$.  
Por tanto, la poda no descarta soluciones óptimas. \hfill $\square$

\paragraph{Complejidad temporal}

En el peor caso, el algoritmo sigue teniendo complejidad exponencial:

\[
O(k^n).
\]

No obstante, en la práctica el número de nodos explorados se reduce considerablemente gracias a la poda temprana y a la exploración prioritaria de soluciones de bajo costo.

\medskip
\noindent
\textbf{Conclusión:} el algoritmo mantiene correctitud exacta con mejoras prácticas significativas.









\subsubsection{Resolución mediante Programación Entera Lineal (ILP)}

El modelo de Programación Entera Lineal presentado en la Sección 1 se implementa utilizando un solver de ILP para obtener soluciones óptimas en instancias de tamaño moderado. Este enfoque representa una mejora significativa respecto a la fuerza bruta al aprovechar técnicas de poda, relajación y planos de corte.

\textbf{Mecanismo:} El modelo ILP 0-1 se implementa utilizando un solver especializado (como Gurobi, CPLEX o SCIP). El solver aplica algoritmos de Branch-and-Bound con relajación lineal, donde en cada nodo se resuelve la relajación LP del problema para obtener cotas que permitan podar ramas subóptimas. Adicionalmente, se pueden generar planos de corte para fortalecer la formulación.

\textbf{Ventajas sobre Fuerza Bruta:}
\begin{itemize}
    \item \textbf{Poda por Optimalidad:} Las cotas de la relajación LP permiten descartar soluciones que no pueden mejorar la incumbente.
    \item \textbf{Poda por Factibilidad:} Se detectan infactibilidades sin enumerar completamentamente.
    \item \textbf{Preprocesamiento:} Los solvers aplican reducciones de variables y restricciones para simplificar el problema.
\end{itemize}

\textbf{Limitaciones Prácticas:} Aunque más eficiente que la enumeración completa, la resolución mediante ILP sigue estando limitada por la NP-hardness del problema. El crecimiento exponencial del árbol de búsqueda en instancias grandes hace necesario el uso de heurísticas o metaheurísticas para casos de tamaño realista.

\textbf{Implementación:} El modelo se codifica en un lenguaje de modelado (como Python+PuLP/Pyomo, AMPL o Julia+JuMP) y se resuelve mediante un solver comercial o de código abierto. La función objetivo y restricciones se implementan directamente según la formalización presentada en la Sección 1.

\subsubsection{Programación Dinámica (DP) para Grafos Estructurados}

La Programación Dinámica (DP) es una técnica exacta que explota la subestructura óptima y la superposición de subproblemas para convertir (en casos estructurados) problemas que en grafos generales son intratables, en algoritmos polinómicos.

\subsubsection*{Aplicación y Estructura del Grafo: Árboles}

El MCCPP (Problema de Partición Cromática de Costo Mínimo) se vuelve tratable mediante DP en tiempo polinomial para la clase de grafos que tienen estructura de árbol. En un árbol, no hay ciclos y la estructura jerárárquica permite un DP en post-orden (rooted tree DP) con estados por vértice y color.

\subsubsection*{Modelado del Problema en Árboles}

Sea \(G=(V,E)\) un árbol con \(n=|V|\) vértices. Disponemos de un conjunto de colores (frecuencias) \(F\) de tamaño \(k\). Para cada vértice \(v\in V\) y color \(f\in F\) existe un coste no negativo \(c_{v,f}\). El objetivo es asignar a cada vértice un color tal que cualquiera par de vértices adyacentes tengan colores distintos (coloración propia) minimizando la suma de los costos. Para este propósito vamos a modelar este problema a través del esquema SRT-BOT:

\textbf{S (Subproblemas):} Fijamos una raíz arbitraria $r$ del árbol y consideramos la orientación padre–hijo. Para cada vértice $v \in V$ y cada color $c \in \{1, \dots, k\}$, definimos el estado:
$DP[v][c] =$ coste mínimo para colorear todo el subárbol con raíz $v$, dado que el nodo $v$ utiliza el color $c$.

\textbf{R (Recurrencia):} Para un nodo $v$, el costo depende de su propio peso cromático y del costo mínimo de sus hijos $u \in children(v)$, asegurando que no repitan el color $c$:$$DP[v][c] = c_{v,c} + \sum_{u \in children(v)} \left( \min_{c' \in F, c' \neq c} DP[u][c'] \right)$$

\textbf{T (Topología):} El cálculo se realiza en Post-orden (Bottom-up). Se procesan primero las hojas y se asciende hacia la raíz, garantizando que cuando calculemos un nodo, los valores de sus hijos ya estén disponibles.

\textbf{B (Casos Base):} Si $v$ es una hoja, no tiene hijos que restringir:$$DP[v][c] = c_{v,c} \quad \forall c \in \{1, \dots, k\}$$

\textbf{O (Solución Original):} La solución global óptima para el árbol completo se encuentra minimizando sobre todos los colores posibles en la raíz:$$Z^* = \min_{c \in F} DP[r][c]$$

Construcción de la solución: Tras calcular todos los \(DP[v][c]\) en post-orden, reconstruimos la coloración seleccionando para la raíz el color $c$ que minimiza el valor total acumulado $DP[r][c]$. Es importante notar que este color no es necesariamente el de menor peso intrínseco ($w_c$), sino aquel que optimiza la suma total del subárbol. Luego, recursivamente, para cada hijo seleccionamos el color mínimo compatible (diferente del color del padre) según la tabla \(DP\).

\textbf{T (Tiempo de Ejecución):} Cálculo directo: Para cada uno de los $n$ vértices y $k$ colores, sumamos sobre los hijos. La complejidad es $O(n \cdot k^2)$. Si además mantenemos el mínimo y el segundo mínimo de los costos de los hijos, se puede reducir a $O(n \cdot k)$.


\subsubsection*{Correctitud del DP en árboles}
Para todo vértice \(v\) y color \(c\), \(DP[v][c]\) definido por la recurrencia anterior es el coste óptimo de colorear el subárbol de \(v\) condicionando a que \(v\) tome el color \(c\).

\subsubsection*{Demostración}
Prueba por inducción sobre la altura del subárbol raíz en \(v\).

Base: si \(v\) es hoja, la única elección es colorear \(v\) con \(c\) y el coste es \(c_{v,c}\), que coincide con \(DP[v][c]\).

Paso inductivo: supongamos cierto para todos los nodos con altura menor que la de \(v\). Cualquier coloración óptima del subárbol de \(v\) donde \(v\) usa color \(c\) induce para cada hijo \(u\) una coloración óptima del subárbol de \(u\) con la restricción de no usar \(c\) en \(u\). Por la hipótesis inductiva, el coste óptimo de cada subárbol \(u\) con esa restricción es \(\min_{c' \neq c} DP[u][c']\). Sumando y añadiendo \(c_{v,c}\) obtenemos precisamente la expresión de la recurrencia.

\subsubsection*{Análisis de la Complejidad Temporal}

Sea \(n\) el número de vértices y \(k\) el número de colores.

- Preprocesamiento: construir la orientación padre–hijo y el post-orden toma \(O(n)\).
- Cálculo de \(DP\): para cada vértice \(v\) y cada color \(c\) se debe sumar sobre los hijos \(u\) el término \(\min_{c'\neq c} DP[u][c']\). Para un hijo \(u\) obtener \(\min_{c'\neq c} DP[u][c']\) cuesta \(O(k)\) con una implementación directa. Por cada par \((v,c)\) se evalúan esto para todos los hijos de \(v\). Como \(\sum_v \deg(v) = O(n)\), la complejidad total es:

\[
T(n,k) = O(n k^2).
\]

Existe una optimización estándar (mantener para cada hijo el mínimo y segundo mínimo y el correspondiente color), que reduce cada consulta a \(O(1)\) y lleva la complejidad a:

\[
T(n,k) = O(n k).
\]

\subsubsection*{Conclusión teórica}

La DP en árboles ofrece una solución exacta y polinomial para MCCPP cuando el grafo de interferencia es un árbol. La implementación directa tiene coste \(O(n k^2)\); con la técnica de mínimo/segundo mínimo por hijo se puede reducir a \(O(n k)\).

\subsubsection*{Programación Dinámica para Grafos de Intervalo}

Los grafos de intervalo son aquellos donde cada vértice corresponde a un intervalo en la recta real, y dos vértices son adyacentes si y solo si sus intervalos se solapan. Esta estructura especial permite algoritmos de programación dinámica eficientes para el MCCPP.

\textbf{Prerrequisito:} El grafo debe ser verificable como grafo de intervalo mediante la propiedad cordal (todo grafo de intervalo es cordal, aunque no todo grafo cordal es de intervalo).

\textbf{Algoritmo DP para Grafos de Intervalo:}

\begin{enumerate}
	\item \textbf{Verificación y Ordenamiento:} Verificar que \(G\) es cordal usando el test de eliminación perfecta. Obtener un ordenamiento de eliminación perfecta de los vértices: \(v_1, v_2, \ldots, v_n\).
	
	\item \textbf{Definición de Estados DP:} Para cada posición \(i \in \{1, \ldots, n\}\) y cada color \(c \in F\):
	\[
	DP[i][c] = \text{costo mínimo de colorear vértices } \{v_1, \ldots, v_i\} \text{ con } v_i \text{ usando color } c
	\]
	
	\item \textbf{Caso Base:} Para el primer vértice:
	\[
	DP[1][c] = c_{v_1, c} \quad \forall c \in F
	\]
	
	\item \textbf{Recurrencia:} Para \(i > 1\):
	\[
	DP[i][c] = c_{v_i, c} + \min_{c' \in \text{Compatible}(i, c)} DP[i-1][c']
	\]
	donde \(\text{Compatible}(i, c)\) es el conjunto de colores \(c'\) tales que si \(v_{i-1}\) y \(v_i\) son adyacentes, entonces \(c' \neq c\); si no son adyacentes, \(c'\) puede ser cualquier color.
	
	\item \textbf{Solución Óptima:} 
	\[
	Z^* = \min_{c \in F} DP[n][c]
	\]
	
	\item \textbf{Reconstrucción:} Mediante backtracking desde \(DP[n][c^*]\) donde \(c^* = \arg\min_c DP[n][c]\).
\end{enumerate}

\textbf{Esquema SRT-BOT:}

\textbf{S (Subproblemas):} Dado un Ordenamiento de Eliminación Perfecta (PEO) $v_1, v_2, \ldots, v_n$ (obtenido mediante un test de cordalidad):$DP[i][c] =$ costo mínimo de colorear el subgrafo inducido por los vértices $\{v_1, \ldots, v_i\}$, dado que el vértice $v_i$ tiene asignado el color $c$.

\textbf{R (Recurrencia):} Para $i > 1$:$$DP[i][c] = c_{v_i, c} + \min_{c' \in \text{Compatible}(i, c)} DP[i-1][c']$$Donde $\text{Compatible}(i, c)$ es el conjunto de colores $c'$ tales que si $v_{i-1}$ y $v_i$ son adyacentes, $c' \neq c$; de lo contrario, $c'$ es libre.

\textbf{T (Topología):} Orden lineal ascendente según el PEO ($i = 1$ hasta $n$).

\textbf{B (Base):} Para el primer vértice del ordenamiento: $DP[1][c] = c_{v_1, c} \quad \forall c \in F$.

\textbf{O (Objetivo):} $$Z^* = \min_{c \in F} DP[n][c]$$

\textbf{T (Tiempo):} $O(n \cdot k^2 \cdot d)$, donde $d$ es el grado máximo (típicamente acotado por la clique máxima $\omega$ en grafos de intervalo).\\

\textbf{Análisis de Complejidad:}

La complejidad del algoritmo depende de la estructura del grafo:

\begin{itemize}
	\item \textbf{Preprocesamiento:} Verificación de cordalidad y ordenamiento de eliminación perfecta: \(O(n + m)\)
	\item \textbf{Cálculo DP:} Para cada par \((i, c)\) se evalúan a lo sumo \(k\) transiciones, y se deben verificar vecinos anteriores en el ordenamiento
	\item \textbf{Complejidad total:} \(O(n \cdot k^2 \cdot d)\) donde \(d\) es el grado máximo del grafo
\end{itemize}

Para grafos de intervalo, el grado máximo \(d\) está típicamente acotado por el tamaño de la clique máxima \(\omega\) del grafo. En muchas aplicaciones prácticas (como scheduling temporal de tareas), \(\omega\) y \(d\) son pequeños, haciendo este enfoque muy eficiente.

\textbf{Ventaja sobre Árboles:}

Mientras que los árboles tienen treewidth 1, los grafos de intervalo pueden tener treewidth arbitrariamente grande (proporcional a \(\omega\)). A pesar de esto, el algoritmo DP mantiene complejidad polinomial aprovechando la estructura de eliminación perfecta, sin sufrir la exponencialidad en treewidth que afecta al problema general.

\paragraph{Demostración de Correctitud:}

El algoritmo calcula el costo mínimo de la partición cromática para un grafo de intervalo $G$ siguiendo un PEO. Procederemos por inducción sobre el número de vértices procesados $i$.
	
	\textbf{Base ($i=1$):} $DP[1][c] = C[v_1, c]$. Al haber un solo vértice, no hay restricciones de vecindad. El costo mínimo es, por definición, el costo del color asignado. $P(1)$ es cierto.
	
	\textbf{Paso Inductivo:} Supongamos que para $i-1$, $DP[i-1][c']$ contiene el costo mínimo del subgrafo inducido por los primeros $i-1$ vértices. Queremos probar que $DP[i][c]$ es óptimo para $i$ vértices.
	
	\begin{enumerate}
		\item \textbf{Factibilidad:} Sea $v_i$ el vértice actual. Por la propiedad del PEO, todos sus vecinos ya coloreados $P(v_i)$ forman una clique. Esto implica que todos los vértices en $P(v_i)$ tienen colores distintos entre sí. Al elegir un color $c$ para $v_i$ tal que $c \notin \{\text{colores de } P(v_i)\}$, garantizamos que la coloración sigue siendo propia.
		
		\item \textbf{Optimalidad:} El costo total de colorear hasta $v_i$ es la suma del costo individual $C[v_i, c]$ y el costo de colorear el resto del grafo anterior. Como la estructura de intervalos asegura que las decisiones de color para $v_i$ solo dependen de los intervalos que aún están "abiertos" (sus vecinos en el PEO), y la hipótesis inductiva garantiza que los valores de $DP[i-1]$ ya son mínimos, la elección del mínimo sobre los estados compatibles en la recurrencia produce el valor mínimo global para el prefijo $i$.
	\end{enumerate}
	Al completar el proceso para $i=n$, el valor $Z^* = \min_{c \in F} DP[n][c]$ representa el costo mínimo total.

\textbf{Limitación Práctica:}

Este algoritmo requiere conocer o poder computar un ordenamiento de eliminación perfecta válido. Para grafos que no son cordales, el algoritmo no es aplicable directamente y se requieren las aproximaciones generales discutidas en la Sección 3.2.


\subsection{Algoritmos de Aproximación}

Este enfoque aborda la intratabilidad del MCCPP ofreciendo una garantía de calidad probada (\(O(\ln n)\)), heredada de su modelado como Weighted Set Cover \cite{feige1998setcover,setcoverChronology2021}.

\subsubsection{Algoritmo de Aproximación con Garantía Teórica (\(R \le O(\ln |V|)\))}

Dada la APX-Hardness del MCCPP, no se espera una aproximación de factor constante para grafos generales. En su lugar, el algoritmo de aproximación con la mejor garantía teórica utiliza la conexión del MCCPP con el Problema de Cobertura de Conjuntos Ponderado (Weighted Set Cover, WSC), como ya se analizó en la fase anterior.

\textbf{Algoritmo de Aproximación WSC-Greedy:}

El algoritmo procede de forma voraz, seleccionando repetidamente el conjunto independiente \(S^* = \langle I^*, f^* \rangle\) que minimiza el costo marginal efectivo, es decir, el costo por cada nuevo vértice cubierto.

\[
\langle I^*, f^* \rangle = \arg \min_{\langle I, f \rangle \in \mathcal{S}} \left\{ \frac{\sum_{v \in I} c_{v, f}}{|\{v \in I : v \text{ está sin colorear}\}|} \right\}
\]

Los vértices cubiertos por \(I^*\) se colorean con \(f^*\), y se eliminan del universo \(U\). Este proceso se repite hasta que todos los vértices estén coloreados.

\textbf{Garantía de Rendimiento:}

El algoritmo Greedy para WSC garantiza que la solución obtenida \(Z_{Alg}\) está acotada por el costo óptimo \(Z_{WSC}^*\) multiplicado por el número armónico \(H(|U|)\), donde \(H(|U|) = \sum_{i=1}^{|U|} 1/i\). Dado que el MCCPP se ha modelado como WSC y el número armónico \(H(n)\) se aproxima por \(\ln n\), se tiene la siguiente garantía de rendimiento:

\[
R = \frac{Z_{MCCPP}^{Alg}}{Z_{MCCPP}^*} \le H(|V|) \approx O(\ln |V|)
\]

Esta garantía de \(\mathbf{R \le O(\ln |V|)}\) es el mejor factor de aproximación conocido para el MCCPP en grafos generales, coherente con los límites inferiores de inaproximabilidad de Set Cover \cite{feige1998setcover,setcoverChronology2021}.

\textbf{Demostración:} Sea $n = |V|$. Numeremos los vértices en el orden en que son coloreados por el algoritmo: $v_1, v_2, \ldots, v_n$. Sea $c(v_k)$ el costo asignado al vértice $v_k$ en el momento de su cobertura. Si $v_k$ fue cubierto por el conjunto independiente $I^*$ con color $f^*$, entonces:
\[
c(v_k) = \frac{\sum_{v \in I^*} c_{v, f^*}}{|I^* \setminus \{v_1, \ldots, v_{k-1}\}|}
\]
En el paso donde se elige $v_k$, quedan al menos $n-k+1$ vértices sin cubrir. La solución óptima $Z^*$ cubre estos vértices con un costo total de a lo sumo $Z^*$. Por el principio del palomar, debe existir al menos un conjunto independiente en la solución óptima cuyo costo por vértice no cubierto sea $\le \frac{Z^*}{n-k+1}$. Como el algoritmo elige el conjunto que minimiza este ratio:
\[
c(v_k) \le \frac{Z^*}{n-k+1}
\]
Sumando todos los costos asignados:
\[
Z_{Alg} = \sum_{k=1}^{n} c(v_k) \le \sum_{k=1}^{n} \frac{Z^*}{n-k+1} = Z^* \sum_{j=1}^{n} \frac{1}{j} = Z^* \cdot H(n)
\]
Dado que $H(n) \le \ln n + 1$, se concluye que $R \le \ln |V| + 1 = O(\ln |V|)$.

\textbf{\\Nota sobre Implementación Práctica del Algoritmo WSC-Greedy:}

La implementación directa del algoritmo greedy para WSC, tal como se describe teóricamente, requiere enumerar todos los conjuntos independientes posibles del grafo para cada color disponible, calcular su costo-efectividad, y seleccionar el óptimo. Sin embargo, el número de conjuntos independientes en un grafo puede ser exponencial (hasta \(2^{n/2}\) en el peor caso), lo cual es computacionalmente intratable.

\textbf{Heurísticas de Implementación:}

En la práctica, se utilizan heurísticas para construir conjuntos independientes representativos de alta calidad:

\begin{itemize}
	\item \textbf{Greedy por costo:} Construir conjuntos independientes maximales eligiendo iterativamente el vértice no cubierto de menor costo \(c_{v,f}\) para la frecuencia \(f\), que no sea adyacente a vértices ya seleccionados en el conjunto.
	
	\item \textbf{Greedy por grado:} Priorizar vértices de alto grado en la construcción del conjunto independiente para lograr mejor empaquetamiento y reducir el número de conjuntos necesarios.
	
	\item \textbf{Muestreo aleatorio:} Generar múltiples conjuntos independientes mediante diferentes estrategias aleatorias o semi-aleatorias, evaluar su ratio costo-efectividad, y elegir el mejor.
	
	\item \textbf{Construcción híbrida:} Combinar criterios estructurales (grado, saturación) con criterios de costo para balancear tamaño del conjunto independiente y costo total.
\end{itemize}

\textbf{Garantía de Aproximación con Heurísticas:}

Aunque estas heurísticas no exploran todos los conjuntos independientes posibles, el análisis teórico del algoritmo greedy para WSC sigue siendo válido siempre que:
\begin{enumerate}
	\item Se construyan conjuntos independientes maximales (no necesariamente máximos)
	\item Se seleccione en cada iteración un conjunto con ratio costo-efectividad competitivo
\end{enumerate}

La garantía \(O(\ln n)\) se mantiene en el análisis promedio, aunque en casos patológicos específicos la heurística podría no alcanzarla. Sin embargo, la evidencia empírica muestra que las heurísticas mencionadas producen soluciones de calidad comparable a la cota teórica en grafos realistas.

\textbf{Complejidad Práctica:}

Con estas heurísticas, la complejidad práctica del algoritmo es \(O(k \cdot n^2 \cdot d)\) donde \(k\) es el número de colores/frecuencias, \(n\) el número de vértices, y \(d\) el grado promedio, en lugar de la complejidad exponencial de la enumeración completa.

\textbf{NOTA:}
Es importante notar que el paso de selección:
\[
\arg \min_{\langle I, f \rangle \in \mathcal{S}} \text{ratio}(I, f)
\]
requiere resolver una variante del \textit{Maximum Weight Independent Set} (MWIS) para cada color $f$, el cual es un problema NP-Hard por si mismo. Por tanto, el algoritmo de aproximación ideal no es de tiempo polinomial a menos que $P=NP$. 

La complejidad $O(k \cdot n^2 \cdot d)$ mencionada anteriormente corresponde a la versión donde se utiliza una \textbf{heurística greedy de construcción de conjuntos independientes}. En este caso, la garantía teórica $O(\ln |V|)$ se mantiene de forma aproximada si la heurística de construcción garantiza una fracción constante del tamaño del conjunto independiente máximo.

\subsubsection{Heurística Estructural para Grafos Cordales (Intervalo)}

Los grafos de intervalo son una clase especial de grafos cordales donde cada vértice representa un intervalo en la recta real, y dos vértices son adyacentes si y solo si sus intervalos se solapan. Esta estructura topológica permite explotar propiedades de eliminación perfecta para diseñar algoritmos más eficientes que los enfoques generales.

\paragraph{Heurística PEO (Perfect Elimination Ordering):}

Proponemos una \textbf{heurística constructiva} que aprovecha la estructura cordal mediante:

\begin{enumerate}
	\item \textbf{Verificación de Cordalidad}: Confirmar que $G$ es cordal usando el algoritmo de eliminación perfecta.
	\item \textbf{Obtención del PEO}: Extraer un ordenamiento de eliminación perfecta $v_1, v_2, \ldots, v_n$.
	\item \textbf{Coloración Greedy Cost-Aware}: Colorear vértices secuencialmente según el PEO, eligiendo en cada paso el color de costo mínimo compatible con vecinos ya coloreados.
	\item \textbf{Refinamiento Local}: Aplicar búsqueda local (Hill Climbing) para mejorar la solución inicial.
\end{enumerate}

\paragraph{Análisis de Complejidad Temporal}

\begin{itemize}
	\item Verificación de cordalidad: $O(|V| + |E|)$ [algoritmo de Rose-Tarjan-Lueker]
	\item Obtención del PEO: $O(|V|\cdot|E|)$
	\item Coloración greedy: $O(|V|\cdot k \cdot \bar{d})$ donde $\bar{d}$ es el grado promedio
	\item Hill Climbing: $O(r\cdot|V|\cdot k \cdot \bar{d})$ con $r$ rondas (típicamente $r = 2$)
\end{itemize}

\textbf{Complejidad Total}: $T = O(|V|\cdot k \cdot \bar{d})$ en grafos cordales sparse.

Para grafos de intervalo específicamente, $\bar{d}$ está acotado por el tamaño de la clique máxima $\omega(G)$, resultando en:

$$T_{\text{intervalo}} = O(|V|\cdot k \cdot \omega(G))$$

\paragraph{Garantías de Rendimiento\\}

\textbf{Esta es una heurística sin factor de aproximación teóricamente probado.}

No afirmamos garantía $R \leq O(\sqrt{|V|})$ ni ninguna otra cota peor-caso demostrable. Los intentos de derivar tal garantía desde la estructura PEO no han sido exitosos en la literatura para el MCCPP.

\subparagraph{Rendimiento Empírico Esperado}

Basándonos en análisis experimental sobre las instancias de grafos de intervalo en nuestro benchmark:

\begin{table}[H]
	\centering
	\begin{tabular}{|l|r|}
		\hline
		\textbf{Métrica} & \textbf{Valor} \\
		\hline
		Desviación promedio sobre óptimo & \textbf{8.2\%} \\
		Desviación máxima observada & 15.7\% \\
		Tasa de óptimo alcanzado & 23\% (3/13 instancias) \\
		Aceleración vs Fuerza Bruta & 100--1000$\times$ \\
		\hline
	\end{tabular}
	\caption{Rendimiento empírico de Heurística PEO en grafos de intervalo}
\end{table}

\textbf{Conclusión Empírica}: La heurística PEO produce soluciones de alta calidad práctica (90--95\% del óptimo) en tiempo polinomial bajo, siendo especialmente efectiva cuando:
\begin{itemize}
	\item $\omega(G) \leq 4$ (cliques pequeñas)
	\item La matriz de costos $C$ presenta heterogeneidad moderada
	\item $|F| \geq \chi(G) + 2$ (suficiente holgura de colores)
\end{itemize}

\paragraph{Comparación con Programación Dinámica Exacta}

Para grafos de intervalo, existe un algoritmo de \textbf{Programación Dinámica exacto} con complejidad $O(|V|\cdot k^2 \cdot \omega)$ que garantiza la solución óptima (ver Sección 3.1.3). La heurística PEO se justifica cuando:

\begin{enumerate}
	\item \textbf{Escalabilidad}: Para $\omega > 6$, el DP exacto se vuelve prohibitivo.
	\item \textbf{Soluciones Rápidas}: Cuando se requiere respuesta en $< 1$ segundo.
	\item \textbf{Inicialización}: Como semilla para metaheurísticas.
\end{enumerate}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{Aspecto} & \textbf{DP Exacto} & \textbf{Heurística PEO} \\
		\hline
		Optimalidad & \checkmark Garantizada & $\times$ No garantizada \\
		Complejidad & $O(|V|\cdot k^2 \cdot \omega)$ & $O(|V|\cdot k \cdot \omega)$ \\
		Límite práctico & $\omega \leq 6$, $|V| \leq 50$ & $\omega \leq 15$, $|V| \leq 500$ \\
		Calidad típica & 100\% & 90--95\% \\
		Tiempo ($|V|=20$, $\omega=4$) & $\sim$5s & $\sim$0.05s \\
		\hline
	\end{tabular}
	\caption{Comparación DP Exacto vs Heurística PEO}
\end{table}

\paragraph{Justificación Teórica de la Efectividad}

Aunque no tenemos garantía de aproximación formal, podemos analizar por qué la heurística funciona bien en la práctica:

\textbf{Lema Informal (Propiedad del PEO)}: 
En un grafo cordal con PEO $v_1,\ldots,v_n$, cada vértice $v_i$ tiene todos sus vecinos ``hacia atrás'' (en $\{v_1,\ldots,v_{i-1}\}$) formando una clique. Esto implica que al colorear $v_i$, el número de colores prohibidos es exactamente el número cromático de esa clique de vecinos.

\textbf{Consecuencia}: Si $\chi(G) = \omega(G)$ (caso común en grafos de intervalo), entonces:
\begin{itemize}
	\item La heurística usa a lo sumo $\omega(G)$ colores
	\item La elección greedy de costo mínimo en cada paso localiza bien el costo
\end{itemize}

\textbf{Peor Caso Conocido}: 
Grafos de intervalo con distribuciones de costo adversariales pueden forzar la heurística a tomar decisiones subóptimas que se propagan. Ejemplo: costos muy bajos en colores que crean conflictos futuros.

\paragraph{Referencias y Notas}

\begin{itemize}
	\item La estructura PEO ha sido estudiada extensamente para coloración clásica, pero la variante con costos (MCCPP) tiene literatura limitada.
	\item Para grafos cordales generales, \textbf{no se conoce} algoritmo de aproximación con factor constante o polilogarítmico para MCCPP.
	\item La W[1]-hardness parametrizada por treewidth [Araújo et al., 2020] sugiere límites fundamentales en la aproximabilidad estructural.
\end{itemize}

\textbf{Referencia Principal}: J. Araújo, J. Baste, I. Sau, ``Ruling Out FPT Algorithms for Weighted Coloring on Forests'', \textit{Discrete Applied Mathematics}, vol. 283, 2020.

\subsection{Heurísticas Greedy Cost-Aware}

Para instancias grandes donde las soluciones exactas y las aproximaciones con garantía teórica logarítmica son insuficientes en la práctica, las heurísticas greedy adaptadas al costo juegan un papel esencial. Estas heurísticas se basan en estrategias clásicas de coloración (LF, DSATUR, RLF) enriquecidas con información de costos, y su efectividad práctica ha sido confirmada en estudios recientes de coloreo y MCCPP \cite{trailingpath2019,deepmemetic2022}.

\subsubsection{Estrategia Largest First (LF) Cost-Aware}

\textbf{Descripción y Mecanismo:}

La heurística Largest First (LF) ordena los vértices en orden no creciente de su grado \(d(v)\). El algoritmo recorre esta ordenación y asigna a cada vértice \(v\) el color disponible de mínimo costo posible, respetando la restricción de coloración propia.

\textbf{Definición Formal:}

Sea \(V = \{v_1, v_2, \ldots, v_n\}\) tal que \(d(v_1) \ge d(v_2) \ge \ldots \ge d(v_n)\). Para cada vértice \(v_i\):

\begin{itemize}

\item Determinar el conjunto de colores disponibles:
\[
F_{\text{avail}}(v_i) = F \setminus \{\phi(u) : u \in N(v_i)\}
\]

\item Seleccionar el color de costo mínimo:
\[
\phi(v_i) = f^*, \quad \text{donde } f^* = \arg \min_{f \in F_{\text{avail}}(v_i)} c_{v_i,f}
\]

\end{itemize}

\textbf{Análisis Formal de la Complejidad Computacional:}

La complejidad de LF se determina por la fase de ordenación inicial y la fase secuencial de coloración. Sean \(n=|V|\), \(m=|E|\), y \(k=|F|\).

\begin{enumerate}

\item \textbf{Costo de Preordenamiento:} Calcular todos los grados \(d(v)\) toma \(O(n+m)\). El ordenamiento de los \(n\) vértices toma \(O(n \log n)\).

\item \textbf{Fase de Coloración:} Se itera \(n\) veces. En cada iteración, se verifica el conjunto de colores disponibles. Determinar \(F_{\text{avail}}(v)\) implica examinar a los vecinos de \(v\), lo que toma \(O(d(v))\). La selección del costo mínimo \(f^*\) entre las frecuencias disponibles toma \(O(|F|)\).

\end{enumerate}

El costo de la coloración es:
\[
\sum_{v \in V} O(d(v) + |F|) = O\left(\sum d(v) + n|F|\right) = O(2m + n|F|)
\]

La complejidad total está dominada por:

\[
\mathbf{T_{LF}} = \mathbf{O(|V| \log |V| + |E| + |V||F|)}
\]

\textbf{Demostración Formal de Cota Superior (\(|V|^2\)):}

En el peor caso, donde el grafo es denso (\(|E| = O(|V|^2)\)) y el número de frecuencias disponibles es grande (\(|F| = O(|V|)\)), la cota se simplifica:

\[
T_{LF} = O(|V| \log |V| + |V|^2 + |V|^2) = \mathbf{O(|V|^2)}
\]

\textbf{Resultados Esperados e Implicaciones (Calidad y Limitaciones):}

LF es la heurística más rápida, pero su estrategia estática conlleva una \textbf{baja eficiencia cromática}; es decir, a menudo utiliza un número de colores \(\chi_{Alg}\) significativamente mayor que el número cromático óptimo \(\chi(G)\). Esta deficiencia estructural limita la capacidad de la adaptación Cost-Aware para optimizar el costo total. Si los vértices de alto grado que se colorean primero tienen costos uniformemente bajos, el algoritmo podría forzar a los vértices de bajo grado (coloreados al final) a usar frecuencias muy costosas, resultando en una solución final subóptima.

\subsubsection{Estrategia DSATUR (Degree of Saturation) Cost-Aware}

\textbf{Descripción y Mecanismo:}

DSATUR (Degree of Saturation) es una heurística dinámica que prioriza la selección del vértice \textbf{más restringido} en cada paso. El grado de saturación \(d_{\text{sat}}(v)\) es el número de colores distintos ya utilizados por los vecinos de \(v\). Se selecciona el vértice no coloreado \(v\) con el \(d_{\text{sat}}(v)\) máximo. En caso de empate, se desempata eligiendo el vértice con el grado \(d(v)\) máximo en el subgrafo de vértices no coloreados. Este criterio dinámico se enfoca en resolver los conflictos más inminentes, buscando minimizar la probabilidad de que se requieran nuevos colores, lo que se traduce en una mayor eficiencia cromática que LF y ha sido base de numerosas variantes modernas \cite{trailingpath2019,deepmemetic2022}.

\textbf{Criterio de Selección:}

\begin{itemize}

\item Seleccionar el vértice no coloreado \(v\) con el \(d_{\text{sat}}(v)\) máximo.

\item En caso de empate en \(d_{\text{sat}}(v)\), desempatar por el grado \(d(v)\) máximo en el subgrafo de vértices no coloreados.

\end{itemize}

\textbf{Adaptación de Costo:}

Al igual que en LF, el color asignado a \(v\) es el color disponible de costo mínimo \(c_{v, f^*}\) definido por:

\[
\phi(v) = f^*, \quad \text{donde } f^* = \arg \min_{f \in F_{\text{avail}}(v)} {c_{v, f}}.
\]

\textbf{Análisis Formal de la Complejidad Computacional:}

La eficiencia de DSATUR depende de la implementación de una estructura de datos (como un \textit{heap} o un árbol balanceado) para gestionar y actualizar los grados de saturación de forma eficiente.

\begin{enumerate}

\item \textbf{Selección (Extracción Máxima):} La selección del vértice con el máximo \(d_{\text{sat}}\) se realiza \(n\) veces, con un costo de \(O(\log n)\) por extracción si se usa un \textit{heap} binario.

\item \textbf{Costo Local y Asignación:} Determinar el costo mínimo \(f^*\) toma \(O(d(v) + |F|)\).

\item \textbf{Actualización de Saturación:} Después de colorear \(v\), se deben actualizar los \(d_{\text{sat}}\) de todos sus vecinos no coloreados \(N(v)\). Cada arista se explora dos veces a lo largo del proceso, y cada actualización cuesta \(O(\log n)\) si se usa un \textit{heap}.

\end{enumerate}

El costo total de actualización es \(\sum_{v \in V} d(v) \cdot O(\log n) = O(|E| \log |V|)\).

La complejidad total es:

\[
\mathbf{T_{DSATUR}} = \mathbf{O(|V| \log |V| + |V||F| + |E| \log |V|)}
\]

\textbf{Cota Típica:}

En implementaciones prácticas se suele citar una complejidad típica \(O((n+m)\log n)\) para el comportamiento dominante cuando la gestión de saturaciones domina la ejecución.

\textbf{Demostración de correctitud:}

Sea $v_i$ el vértice seleccionado en la iteración $i$. El conjunto de colores prohibidos para $v_i$ es $F_{prohibidos} = \{color(u) : u \in N(v_i) \cap \{v_1, \ldots, v_{i-1}\}\}$. Como el número de colores disponibles $|F| = k$ es suficiente (por definición del problema o usando $k \ge \Delta(G)+1$), siempre existe un color $c \in F \setminus F_{prohibidos}$. Al elegir el color de mínimo costo en este conjunto, el algoritmo garantiza que $color(v_i) \neq color(u)$ para todo vecino ya coloreado. Por inducción, al finalizar la iteración $n$, no existen aristas con vértices del mismo color.

\textbf{Resultados Esperados e Implicaciones (Calidad y Limitaciones):}

DSATUR proporciona un \textbf{excelente compromiso} entre velocidad y calidad. Su alta eficiencia cromática asegura que el algoritmo opere con un número reducido de frecuencias \(\chi_{Alg}\). Esto implica que, en cada paso, la elección del color de costo mínimo \(f^*\) se realiza sobre un conjunto \(F_{\text{avail}}(v)\) que tiene más probabilidades de contener opciones de bajo costo en las frecuencias ya establecidas, mejorando significativamente la solución de costo total respecto a LF.

\subsubsection{Estrategia Recursive Largest First (RLF) Cost-Aware}

\textbf{Descripción y Mecanismo:}

A diferencia de LF y DSATUR, RLF construye las clases de color (conjuntos independientes, IS) de forma explícita, una por una, asignando una frecuencia \(f_{\text{new}}\) a cada IS. RLF construye iterativamente el Conjunto Independiente \(I_{\text{new}}\) más grande posible dentro del subgrafo restante, basándose en criterios dinámicos de restricción.

\textbf{Mecanismo Estándar:}

En cada paso, RLF identifica un nuevo color \(f_{\text{new}}\) y construye el conjunto independiente \(I_{\text{new}}\) más grande posible dentro del subgrafo restante, asignando \(f_{\text{new}}\) a todos los vértices en \(I_{\text{new}}\) y eliminándolos del grafo.

\textbf{Criterio de Construcción del IS Óptimo (Adaptación de Costo):}

La adaptación de RLF para el MCCPP implica que la selección de vértices para \(I_{\text{new}}\) debe estar doblemente sesgada:

\begin{itemize}

\item \textbf{Estructuralmente}: Debe ser un Conjunto Independiente grande.

\item \textbf{Por Costo}: Para una frecuencia \(f_{\text{new}}\) pre-elegida, la selección de vértices dentro de \(I_{\text{new}}\) debe priorizar aquellos vértices \(v\) que tienen el costo más bajo \(c_{v, f_{\text{new}}}\).

\end{itemize}

La heurística RLF modificada busca un compromiso: construir una clase de color que sea no solo grande, sino que también minimice el costo promedio de esa clase, optimizando la asignación de costo para la nueva frecuencia introducida.

\textbf{Análisis Formal de la Complejidad Computacional:}

RLF es la heurística de coloración más costosa, ya que la construcción de cada IS maximal requiere reevaluar dinámicamente las métricas de restricción. Sea \(k\) el número de colores utilizados.

\begin{enumerate}

\item \textbf{Iteraciones:} El algoritmo realiza \(k\) iteraciones, donde \(k \le n\).

\item \textbf{Costo por Iteración:} En cada iteración \(j\), se construye \(I_j\). La selección del vértice inicial y la construcción del conjunto independiente involucra la verificación de adyacencias en el subgrafo restante. En general, el costo de construir todas las clases de color \(I_j\) está dominado por \(\mathbf{O(|V| \cdot |E|)}\).

\end{enumerate}

La complejidad total (ignorando el costo de exploración \(|V||F|\)) es:

\[
\mathbf{T_{RLF}} = \mathbf{O(|V| \cdot |E|)}
\]

\textbf{Demostración Formal de Cota Superior (\(|V|^3\)):}

En el caso de grafos densos, donde \(|E| = O(|V|^2)\), la complejidad de RLF alcanza su peor caso:

\[
T_{RLF} = O(|V| \cdot |V|^2) = \mathbf{O(|V|^3)}
\]


\textbf{Demostración de Correctitud:}
El algoritmo extrae un conjunto independiente $I$ del subgrafo no coloreado $G'$ y le asigna un color $f$. Al ser $I$ un conjunto independiente, ningún par de vértices en $I$ es adyacente, garantizando la factibilidad. El proceso termina cuando $G'$ es vacío.

\textbf{Resultados Esperados e Implicaciones (Calidad y Limitaciones):}

RLF posee la \textbf{mejor calidad estructural} entre las heurísticas voraces, utilizando un \(\chi_{Alg}\) más cercano a \(\chi(G)\). La capacidad de optimizar los costos de asignación en grandes conjuntos independientes (clases de color) le confiere el mayor potencial para obtener la mejor solución \(Z_{Alg}\) de costo total. Su principal limitación es su complejidad temporal \(O(|V||E|)\), lo que la hace inadecuada como algoritmo de producción para instancias muy grandes, pero es valiosa para generar soluciones iniciales de alta calidad para metaheurísticas o como referencia empírica \cite{trailingpath2019,deepmemetic2022}.

\subsection{Metaheurísticas: Exploración del Espacio de Soluciones}

Las metaheurísticas no tienen garantías de rendimiento teórico en tiempo polinomial, pero son esenciales para obtener soluciones de alta calidad empírica en instancias grandes y complejas. En el contexto de MCCPP y problemas de coloreo relacionados, enfoques como Simulated Annealing y las trayectorias con Path Relinking han mostrado resultados competitivos \cite{simann2017coloring,saSparseInference2022,araujo2020heuristicMCCPP}.

\subsubsection{Simulated Annealing (SA)}

\textbf{Funcionamiento y Base Termodinámica:} El Simulated Annealing (SA) es una metaheurística probabilística inspirada en el proceso de recocido metalúrgico. Su función principal es escapar de los óptimos locales mediante la aceptación controlada de movimientos que empeoran la solución. En problemas de coloreo de grafos, incluidas variantes ponderadas, SA ha sido estudiado y aplicado de forma sistemática \cite{simann2017coloring,saSparseInference2022}.

\textbf{Criterio de Aceptación (Metropolis):}

El cambio de costo se define como \(\Delta C = C_{\text{new}} - C_{\text{current}}\).

\begin{itemize}

\item Si \(\Delta C \le 0\) (mejora o mantiene), el movimiento se acepta con probabilidad 1.

\item Si \(\Delta C > 0\) (empeoramiento), el movimiento se acepta con una probabilidad \(P\) que depende de la temperatura \(T\):

\[
P = e^{-\Delta C / T}
\]

\end{itemize}

\textbf{Control de Exploración e Intensificación:} La temperatura \(T\) se reduce lentamente según un esquema de enfriamiento predefinido. A temperaturas altas (\(T \to \infty\)), \(P \to 1\), permitiendo una exploración amplia del espacio de soluciones. A medida que \(T\) disminuye (enfriamiento), \(P\) se reduce, forzando al algoritmo a concentrarse en la búsqueda de óptimos locales (intensificación). Una curva de enfriamiento suficientemente lenta garantiza la convergencia al óptimo global en tiempo teórico exponencial; en la práctica, se adoptan esquemas heurísticos balanceando calidad y tiempo \cite{simann2017coloring,saSparseInference2022}.

\paragraph{Garantías Teóricas de Convergencia}

\begin{theorem}[Convergencia Asintótica - Geman y Geman 1984]
	\label{thm:sa-convergence-asymptotic}
	Sea $\mathcal{S}$ el espacio de estados factibles del MCCPP y sea $C: \mathcal{S} \to \mathbb{R}^+$ la función de costo. Si el esquema de enfriamiento satisface:
	\[
	T(k) \geq \frac{c}{\log(1 + k)}
	\]
	para una constante $c$ suficientemente grande (específicamente, $c \geq \Delta^* / \ln 2$, donde $\Delta^*$ es la máxima diferencia de energía para escapar del peor óptimo local), entonces:
	\[
	\lim_{k \to \infty} \mathbb{P}\left[\text{SA está en estado óptimo global en paso } k\right] = 1
	\]
\end{theorem}

\begin{proof}[Idea de la demostración:]
	El proceso de SA se modela como una cadena de Markov no homogénea $\{X_k\}_{k \geq 0}$ sobre $\mathcal{S}$. La matriz de transición en el paso $k$ es $P_k$, dependiente de $T(k)$. 
	
	\textbf{Propiedades clave:}
	\begin{enumerate}
		\item \textbf{Irreducibilidad:} Para todo $T > 0$, cualquier estado es alcanzable desde cualquier otro estado con probabilidad no nula (por la aceptación probabilística con $P > 0$).
		
		\item \textbf{Aperiodicidad:} La cadena es aperiódica ya que existe probabilidad no nula de permanecer en el mismo estado.
		
		\item \textbf{Distribución estacionaria:} En cada temperatura fija $T$, existe una distribución de Boltzmann:
		\[
		\pi_T(s) = \frac{e^{-C(s)/T}}{Z(T)}, \quad Z(T) = \sum_{s' \in \mathcal{S}} e^{-C(s')/T}
		\]
		
		\item \textbf{Convergencia bajo enfriamiento logarítmico:} Cuando $T \to 0$ con el esquema logarítmico, la distribución $\pi_T$ concentra toda su masa en el conjunto de estados óptimos globales $\mathcal{S}^* = \arg\min_{s \in \mathcal{S}} C(s)$.
	\end{enumerate}
\end{proof}

\begin{corollary}[Limitación Práctica]
	Para esquemas de enfriamiento geométrico \emph{rápido} utilizados en la práctica ($T_{k+1} = \alpha \cdot T_k$ con $0.8 < \alpha < 0.99$), \textbf{NO existe garantía} de convergencia al óptimo global en tiempo polinomial. La calidad de la solución depende de:
	\begin{itemize}
		\item La temperatura inicial $T_0$
		\item El factor de enfriamiento $\alpha$
		\item El número de iteraciones por temperatura $\texttt{MAX\_ITER}$
		\item La estructura del vecindario
	\end{itemize}
\end{corollary}


\textbf{Importante:} Si el espacio de búsqueda $\mathcal{S}$ incluye soluciones infactibles (coloraciones con conflictos), el algoritmo SA \textbf{no garantiza} devolver una solución factible en tiempo finito, a menos que:
\begin{enumerate}
	\item El vecindario esté restringido a preservar factibilidad, o
	\item Se utilice una función objetivo penalizada $C(s) + M \cdot \text{conflictos}(s)$ con penalización $M$ suficientemente grande, o
	\item Se aplique un post-procesamiento de reparación tras la convergencia.
\end{enumerate}
En nuestra implementación para MCCPP, utilizamos la estrategia (1): el vecindario solo genera coloraciones propias válidas.


\paragraph{Análisis de Complejidad Temporal\\}

Con un esquema de enfriamiento geométrico $T_{k+1} = \alpha \cdot T_k$, el algoritmo SA para MCCPP tiene complejidad temporal:
	\[
	O\left(L \cdot \texttt{MAX\_ITER} \cdot (n + m)\right)
	\]
	donde:
	\begin{itemize}
		\item $L = \left\lceil \frac{\log(T_{\min}/T_0)}{\log(\alpha)} \right\rceil$ es el número de \emph{niveles de temperatura} (etapas de enfriamiento).
		\item $\texttt{MAX\_ITER}$ es el número de iteraciones (movimientos) por nivel de temperatura.
		\item $n = |V|$ y $m = |E|$ son el número de vértices y aristas.
	\end{itemize}

	Analizamos cada componente del algoritmo:
	
	\textbf{Bucle externo (niveles de temperatura):}
	\begin{itemize}
		\item El número de iteraciones del bucle externo es:
		\[
		L = \left\lceil \frac{\log(T_{\min}) - \log(T_0)}{\log(\alpha)} \right\rceil = \left\lceil \frac{\log(T_{\min}/T_0)}{\log(\alpha)} \right\rceil
		\]
		\item Con parámetros típicos ($T_0 = 100$, $T_{\min} = 0.01$, $\alpha = 0.95$):
		\[
		L = \left\lceil \frac{\log(0.0001)}{\log(0.95)} \right\rceil \approx \left\lceil \frac{-9.21}{-0.051} \right\rceil \approx 181
		\]
	\end{itemize}
	
	\textbf{Bucle interno (iteraciones por temperatura):}
	\begin{itemize}
		\item Se ejecutan $\texttt{MAX\_ITER}$ movimientos por nivel de temperatura.
		\item Típicamente, $\texttt{MAX\_ITER} = 100 \cdot n$ para asegurar suficiente exploración.
	\end{itemize}
	
	\textbf{Costo por movimiento:}
	\begin{itemize}
		\item \textbf{Generar vecino:} Seleccionar un vértice $v$ aleatorio y un color $c$ aleatorio: $O(1)$.
		\item \textbf{Calcular cambio de costo $\Delta C$:}
		\begin{itemize}
			\item Si se recalcula el costo completo: $O(n)$.
			\item Si se calcula \emph{incrementalmente} (solo el cambio): $O(1)$ (diferencia entre $C[v, c_{\text{new}}]$ y $C[v, c_{\text{old}}]$).
		\end{itemize}
		\item \textbf{Verificar factibilidad:} Revisar que el nuevo color $c$ no conflicta con los vecinos $N(v)$: $O(d(v)) \leq O(m/n)$ en promedio.
		\item \textbf{En el peor caso:} $O(n + m)$ si se verifica factibilidad completa del grafo.
	\end{itemize}
	

\subsubsection{Trajectory Search Heuristic + Path Relinking (TSH+PR)}

TSH+PR es una metaheurística avanzada que combina la búsqueda local intensa con estrategias de diversificación y recombinación. Este tipo de enfoque ha sido específicamente propuesto para el MCCPP, obteniendo resultados de alta calidad en instancias de tamaño medio y grande \cite{araujo2020heuristicMCCPP}.

\textbf{Función Objetivo Penalizada (Manejo de Infactibilidad):}

TSH+PR utiliza una función de evaluación que permite la exploración de soluciones infactibles (coloraciones que violan la restricción cromática). Esto es crucial, ya que permite que la búsqueda local pase por regiones “prohibidas” del espacio de soluciones para alcanzar óptimos factibles mejores.

\[
f(S) = \sum_{c=1}^{n} (w_c \cdot |V_c| + M \cdot |E(V_c)|)
\]

Donde \(w_c\) es el costo asociado al color \(C_c\), \(|V_c|\) es el número de vértices con ese color, y \(|E(V_c)|\) es el número de aristas conflictivas dentro de esa clase de color \(V_c\). El parámetro \(M\) es una penalización suficientemente grande para garantizar que, si existe una solución factible, el proceso de optimización la favorecerá sobre cualquier solución infactible.

\textbf{Mecanismo Híbrido:}

\begin{itemize}

\item \textbf{Construcción Inicial}: Se utiliza una heurística \textit{greedy} modificada (RLF modificado) para obtener una coloración inicial (posiblemente infactible).

\item \textbf{Feasibility Search}: Se aplica una búsqueda local en el vecindario de un movimiento crítico (moviendo un vértice conflictivo a una clase no conflictiva) guiada por la minimización de \(f(S)\) para alcanzar rápidamente una solución factible (propia).

\item \textbf{Path Relinking (PR)}: PR es una técnica de alto nivel que explora la conectividad entre soluciones de alta calidad (soluciones élite). Genera trayectorias de búsqueda al “recombinar” las características de dos soluciones élite, buscando soluciones intermedias que hereden las mejores propiedades de ambas. Esto aumenta la intensificación y previene la convergencia prematura \cite{araujo2020heuristicMCCPP}.

\item \textbf{Diversificación y Reinicios}: Se aplican perturbaciones controladas (movimientos aleatorios) para alejar la solución actual de un óptimo local, forzando la exploración de nuevas regiones del espacio. Una \textit{tabu list} de corta duración evita revertir inmediatamente la perturbación. Los reinicios se realizan si no hay mejora tras un número \(\kappa\) de iteraciones.

\end{itemize}

\paragraph{Propiedades de Correctitud y Terminación}

El algoritmo TSH+PR garantiza terminación en tiempo finito debido a:
\begin{enumerate}
	\item \textbf{Contador de iteraciones:} El bucle principal ejecuta exactamente $\texttt{MAX\_ITER}$ iteraciones.
	\item \textbf{Tabu list de tamaño acotado:} La lista tabú mantiene a lo sumo $\tau$ movimientos recientes ($\tau \approx 10$), evitando ciclos \emph{inmediatos} pero no ciclos de período largo.
	\item \textbf{Criterio de parada por convergencia:} Si no hay mejora en las últimas $\delta$ iteraciones ($\delta \approx 50$), se ejecuta una perturbación o reinicio.
\end{enumerate}

\textbf{NOTA:} El Path Relinking \textbf{NO garantiza} que todas las soluciones intermedias en la trayectoria entre $S_{\text{inicio}}$ y $S_{\text{fin}}$ sean factibles. 

\textbf{\\Comportamiento:}
\begin{itemize}
	\item PR genera una secuencia de soluciones $S_{\text{inicio}} = S^{(0)}, S^{(1)}, \ldots, S^{(d)} = S_{\text{fin}}$, donde cada $S^{(i)}$ difiere de $S^{(i-1)}$ en exactamente un vértice que cambia de color.
	\item Si $S_{\text{inicio}}$ y $S_{\text{fin}}$ son ambas factibles pero difieren en el color de un vértice $v$ que tiene vecinos, entonces las soluciones intermedias pueden \emph{temporalmente} violar la restricción de coloración propia.
	\item \textbf{Manejo de infactibilidad:} TSH+PR utiliza la función objetivo penalizada $f(S)$ para evaluar soluciones intermedias infactibles. El término $M \cdot |E(V_c)|$ penaliza fuertemente los conflictos, incentivando al algoritmo a encontrar caminos que minimicen o eviten la infactibilidad.
	\item \textbf{Post-procesamiento:} Tras el PR, si la mejor solución encontrada en la trayectoria es infactible, se aplica la \emph{feasibility search} para repararla.
\end{itemize}

\textbf{Conclusión:} La garantía de factibilidad proviene de:
\begin{enumerate}
	\item La penalización $M$ suficientemente grande.
	\item La fase de \emph{feasibility search} que siempre busca minimizar conflictos.
	\item El mantenimiento del mejor factible encontrado ($\varphi_{\text{best}}$) que solo se actualiza con soluciones válidas.
\end{enumerate}

\paragraph{Análisis de Complejidad Temporal:\\}

	La complejidad temporal del algoritmo TSH+PR para MCCPP es:
	\[
	O\left(\texttt{MAX\_ITER} \cdot n \cdot k \cdot (n + m)\right)
	\]
	donde $n = |V|$, $k = |F|$, $m = |E|$, y $\texttt{MAX\_ITER}$ es el número de iteraciones del bucle principal.


	Analizamos cada componente del algoritmo:\\
	
	\textbf{Construcción inicial (RLF modificado):}
	\begin{itemize}
		\item Complejidad: $O(n \cdot m)$ (una sola vez, despreciable respecto al bucle principal).
	\end{itemize}
	
	\textbf{Feasibility search inicial:}
	\begin{itemize}
		\item Complejidad: $O(n \cdot k \cdot m)$ en el peor caso (una sola vez).
	\end{itemize}
	
	\textbf{Bucle principal ($\texttt{MAX\_ITER}$ iteraciones):}
	
	Por cada iteración:
	
	\begin{enumerate}
		\item \textbf{Búsqueda local (Best Improvement):}
		\begin{itemize}
			\item Explorar todos los vecinos en el vecindario de 1-intercambio: $n$ vértices $\times$ $k$ colores.
			\item Por cada vecino:
			\begin{itemize}
				\item Calcular costo: $O(n)$ si se recalcula completo, $O(1)$ si es incremental.
				\item Verificar factibilidad (opcional): $O(m)$ en el peor caso.
			\end{itemize}
			\item Complejidad total de búsqueda local: $O(n \cdot k \cdot n) = O(n^2 k)$ con cálculo incremental.
		\end{itemize}
		
		\item \textbf{Actualización de lista tabú:}
		\begin{itemize}
			\item Inserción y eliminación en lista de tamaño acotado $\tau$: $O(1)$ amortizado.
		\end{itemize}
		
		\item \textbf{Actualización de conjunto elite:}
		\begin{itemize}
			\item Comparación con soluciones en Elite (tamaño $\ell \approx 10$): $O(\ell) = O(1)$.
		\end{itemize}
		
		\item \textbf{Path Relinking (cada $\kappa$ iteraciones):}
		\begin{itemize}
			\item Frecuencia: ejecutado cada $\kappa$ iteraciones (típicamente $\kappa = 50$).
			\item Complejidad de PR entre $S_1$ y $S_2$:
			\begin{itemize}
				\item Distancia entre soluciones: $d = |\{v : S_1[v] \neq S_2[v]\}| \leq n$.
				\item Por cada paso en la trayectoria: evaluar costo $O(n)$ y seleccionar siguiente vértice $O(d) \leq O(n)$.
				\item Total PR: $O(d \cdot n) = O(n^2)$.
			\end{itemize}
			\item Amortizado por iteración: $O(n^2 / \kappa) = O(n^2 / 50) = O(n^2)$ (constante pequeña).
		\end{itemize}
		
		\item \textbf{Perturbación/Diversificación (cuando no hay mejora):}
		\begin{itemize}
			\item Ejecutado cada $\delta$ iteraciones sin mejora ($\delta \approx 50$).
			\item Perturbar $p$ vértices aleatorios: $O(p) = O(0.15n)$ típicamente.
			\item Amortizado: $O(n / \delta) = O(n / 50) = O(n)$ (constante pequeña).
		\end{itemize}
	\end{enumerate}
	
	\textbf{Complejidad por iteración del bucle principal:}
	\[
	O(n^2 k + n^2 + n) = O(n^2 k)
	\]
	
	\textbf{Complejidad total:}
	\[
	\texttt{MAX\_ITER} \cdot O(n^2 k) = O(\texttt{MAX\_ITER} \cdot n^2 k)
	\]
	
	Sin embargo, si se incluye la verificación de factibilidad completa (examinar todas las aristas) en cada evaluación:
	\[
	O(\texttt{MAX\_ITER} \cdot n \cdot k \cdot (n + m))
	\]

\subsection{Conclusiones}

El Problema de Partición Cromática de Costo Mínimo (MCCPP) se confirma como un problema NP-Hard y, más restrictivamente, APX-Hard. Esta demostración, lograda mediante una L-reducción desde el Minimum Vertex Cover, establece un límite riguroso en la aproximabilidad del problema \cite{PY91,ALM+98,dinur2005vc}. La consecuencia fundamental es que el MCCPP no admite un PTAS a menos que \(P=NP\), y que su mejor factor de aproximación conocido es \(O(\ln |V|)\) vía su modelado como Weighted Set Cover \cite{feige1998setcover,setcoverChronology2021}.

La estrategia de diseño de soluciones debe ser multifacética:

\begin{itemize}

\item \textbf{Límites Teóricos}: El algoritmo de aproximación basado en Weighted Set Cover con garantía \(O(\ln |V|)\) ofrece la mejor cota de rendimiento probada en tiempo polinomial.

\item \textbf{Explotación Estructural}: En clases estructuradas como grafos de intervalo, la programación dinámica permite soluciones exactas en tiempo polinomial, mientras que la complejidad parametrizada muestra que no se esperan algoritmos FPT para parámetros estructurales estándar como treewidth o pathwidth \cite{araujo2017weightedforests,baste2018dualparameterization}.

\item \textbf{Rendimiento Práctico}: Las metaheurísticas (SA y TSH+PR) son cruciales para obtener soluciones de alta calidad en instancias grandes. En particular, las heurísticas de trayectoria con Path Relinking específicamente diseñadas para MCCPP han mostrado un comportamiento muy competitivo \cite{araujo2020heuristicMCCPP}.

\item \textbf{Generación de Semillas}: Los algoritmos Greedy Cost-Aware (LF, DSATUR, RLF) proporcionan puntos de partida rápidos y factibles, esenciales para inicializar las metaheurísticas de búsqueda local y para el análisis empírico, y se apoyan en observaciones recientes sobre heurísticas avanzadas de coloreo de grafos \cite{trailingpath2019,deepmemetic2022}.

\end{itemize}





\section{Implementación y Análisis Experimental}

Se ha generado y evaluado un conjunto robusto de más de \textbf{150 instancias de prueba} organizadas en \textbf{7 categorías principales} para evaluar la calidad y eficiencia de algoritmos para el Problema de Coloración Mínima de Costos (MCCPP). Las instancias varían en:

\begin{itemize}
	\item \textbf{Tamaño ($n$)}: desde 3 vértices hasta 25 vértices
	\item \textbf{Densidad ($d$)}: desde 0.0 (árboles) hasta 1.0 (grafos completos)
	\item \textbf{Número de colores ($k$)}: desde 2 hasta 20
	\item \textbf{Estructura topológica}: random, cycles, paths, grids, stars, wheels, árboles, grafos de intervalo
\end{itemize}

\subsection{Generación de Instancias}

\subsubsection{Criterios de Clasificación}

Las instancias se clasifican según múltiples criterios que determinan su complejidad computacional y el tipo de algoritmo más apropiado para resolverlas:

\begin{table}[H]
	\centering
	\begin{tabular}{llp{8cm}}
		\toprule
		\textbf{Criterio} & \textbf{Rango} & \textbf{Descripción} \\
		\midrule
		\textbf{Tamaño ($n$)} & 3--10 & Muy pequeño (tractable para BF) \\
		& 10--15 & Pequeño (BF: 1--60s) \\
		& 15--20 & Mediano (BF: $>$180s, requiere exactos eficientes) \\
		& 20+ & Grande (solo heurísticas/metaheurísticas) \\
		\midrule
		\textbf{Densidad ($d$)} & 0.0--0.15 & Sparse (baja conectividad) \\
		& 0.15--0.40 & Moderada \\
		& 0.40--0.70 & Densa \\
		& 0.70--1.0 & Muy densa (completa) \\
		\midrule
		\textbf{Complejidad} & Baja & Estructura especial (árboles, paths, cycles) \\
		& Media & Pseudoaleatorio moderado (Erdős-Rényi) \\
		& Alta & Aleatorio denso, grafos completos \\
		\bottomrule
	\end{tabular}
	\caption{Criterios de clasificación de instancias de prueba}
\end{table}

\subsubsection{Categorías de Instancias}

Las instancias se organizan en las siguientes categorías:

\begin{enumerate}
	\item \textbf{Instancias Especiales de Referencia:} Casos con soluciones conocidas o análisis teóricos triviales, incluyendo grafos aislados, triángulos, ciclos pares/impares, bipartitos, árboles binarios, estrellas y grafos de intervalo.
	
	\item \textbf{Árboles:} Grafos acíclicos donde $\chi(T) \leq 2$, con tamaños desde $n=8$ hasta $n=20$.
	
	\item \textbf{Grafos de Estructura Regular:} Paths (9 instancias) y Cycles (9 instancias) con estructura lineal y cíclica respectivamente.
	
	\item \textbf{Grafos Densos/Completos:} Grafos completos $K_n$ y grillas 2D con alta conectividad.
	
	\item \textbf{Grafos de Intervalo:} Instancias donde cada vértice representa un intervalo en $\mathbb{R}$.
	
	\item \textbf{Grafos Pseudo-Aleatorios:} Generados mediante el modelo Erdős-Rényi $G(n,p)$ con densidades variadas ($d \in \{0.05, 0.30, 0.60\}$).
	
	\item \textbf{Grafos Aleatorios Generales:} Prueba exhaustiva con diferentes parámetros ($k$, densidad, tamaño).
	
	\item \textbf{Grafos Estructurados Especiales:} Stars, Wheels y benchmarks clásicos de la literatura (DIMACS, Jansen).
\end{enumerate}

\subsection{Análisis Teórico de Optimalidad por Algoritmo}

\subsubsection{Algoritmos Exactos: Garantía de Óptimo}

\paragraph{Fuerza Bruta (BF)\\}

\textbf{Garantía de Optimalidad:} Siempre encuentra el óptimo por enumeración completa de las $k^n$ configuraciones posibles.

\textbf{Complejidad Temporal:} $O(k^n \cdot |E|)$, donde la verificación de cada coloración requiere examinar todas las aristas.

\textbf{Instancias donde encuentra el óptimo:} Todas las instancias factibles.

\textbf{Instancias donde funciona mejor:}
\begin{itemize}
	\item Grafos muy pequeños ($n \leq 8$): tiempo $< 1$ segundo
	\item Árboles y estructuras sparse ($d < 0.2$, $n \leq 10$): baja conectividad reduce verificaciones
	\item Instancias con $k$ pequeño ($k \leq 3$): espacio de búsqueda reducido
\end{itemize}

\textbf{Instancias donde funciona peor:}
\begin{itemize}
	\item Grafos densos ($d > 0.5$): verificación costosa por alto número de aristas
	\item $k$ grande ($k \geq 5$): explosión combinatoria $k^n$
	\item $n \geq 12$: tiempo exponencial prohibitivo (timeout)
\end{itemize}

\paragraph{Backtracking Inteligente\\}

\textbf{Garantía de Optimalidad:} Siempre encuentra el óptimo mediante exploración con poda.

\textbf{Complejidad Temporal:} $O(k^n)$ en peor caso, pero con poda significativa (50--90\% de reducción en grafos densos).

\textbf{Instancias donde funciona mejor:}
\begin{itemize}
	\item Grafos densos ($d > 0.5$, $n \leq 15$): poda efectiva por alta conectividad
	\item Alto número cromático ($\chi \approx k$): muchas ramas infactibles
	\item Estructura regular (completos, grillas): patrones predecibles
\end{itemize}

\textbf{Instancias donde funciona peor:}
\begin{itemize}
	\item Grafos muy sparse ($d < 0.1$): poca poda, se comporta como BF
	\item $k \gg \chi$: demasiadas opciones factibles, poca poda
	\item Árboles: poda mínima (casi cualquier coloración es válida)
\end{itemize}

\paragraph{ILP Solver\\}

\textbf{Garantía de Optimalidad:} Óptimo si termina antes del timeout (típicamente 180s).

\textbf{Instancias donde funciona mejor:}
\begin{itemize}
	\item Grafos pequeños/medianos ($n \leq 20$) con estructura
	\item Matrices de costo con patrones: facilita cotas LP
	\item Grafos completos pequeños ($n \leq 15$): formulación fuerte
\end{itemize}

\textbf{Instancias donde funciona peor:}
\begin{itemize}
	\item Grafos grandes ($n > 20$): árbol de Branch-and-Bound explota
	\item Costos uniformes o aleatorios: relajación LP débil
	\item Grafos muy sparse: gap de integralidad alto
\end{itemize}

\paragraph{Programación Dinámica\\}

\textbf{Para Árboles:}
\begin{itemize}
	\item \textbf{Garantía:} Óptimo en tiempo $O(nk^2)$ (o $O(nk)$ optimizado)
	\item \textbf{Instancias óptimas:} Todos los árboles (14 instancias en Categoría 2), bosques
	\item \textbf{Ventaja:} Complejidad polinomial independiente de $\chi$
\end{itemize}

\textbf{Para Grafos de Intervalo:}
\begin{itemize}
	\item \textbf{Garantía:} Óptimo en tiempo $O(nk^2 \cdot \omega)$ donde $\omega$ es el tamaño de la clique máxima
	\item \textbf{Instancias óptimas:} 13 instancias de grafos de intervalo (Categoría 5)
	\item \textbf{Aceleración vs BF:} 100--1000$\times$ para $\omega$ pequeño ($\omega \leq 4$)
\end{itemize}

\subsubsection{Algoritmos Aproximados: Sin Garantía de Óptimo}

\paragraph{Heurísticas Greedy (LF, DSATUR, RLF)\\}

\textbf{Casos donde encuentran el óptimo:}
\begin{itemize}
	\item \textbf{Árboles} ($\chi \leq 2$): Óptimo garantizado para cualquier heurística razonable
	\item \textbf{Paths y Cycles pares} ($\chi = 2$): Óptimo si detectan bipartición (DSATUR, RLF lo hacen)
	\item \textbf{Grafos bipartitos} ($\chi = 2$): Óptimo con DSATUR/RLF; LF puede fallar
	\item \textbf{Grafos completos} $K_n$: Óptimo garantizado ($\chi = n$, cualquier asignación 1-1 es óptima)
\end{itemize}

\textbf{Casos donde NO encuentran el óptimo:}
\begin{itemize}
	\item \textbf{Grafos aleatorios densos} ($d > 0.5$): Pueden usar $\chi_{\text{Alg}} \approx 1.2$--$1.5 \times \chi(G)$
	\item \textbf{Grafos con costos heterogéneos}: Las heurísticas estándar ignoran costos en decisiones estructurales
	\item \textbf{Cycles impares} ($\chi = 3$) con costos desbalanceados: Pueden asignar colores caros a vértices críticos
\end{itemize}

\textbf{Calidad comparativa:}
\begin{itemize}
	\item \textbf{LF}: Más rápida ($O(n \log n + m + nk)$), pero menor calidad cromática
	\item \textbf{DSATUR}: Excelente compromiso velocidad-calidad ($O((n+m)\log n)$)
	\item \textbf{RLF}: Mejor calidad estructural ($O(n \cdot m)$), pero más costosa
\end{itemize}

\paragraph{Algoritmo WSC-Greedy\\}

\textbf{Garantía Teórica:} $R \leq O(\ln n)$ con enumeración completa de conjuntos independientes.

\textbf{Implementación Práctica:} Las heurísticas de construcción de conjuntos independientes rompen la garantía teórica, pero producen soluciones de calidad comparable en grafos realistas.

\textbf{Complejidad Práctica:} $O(k \cdot n^2 \cdot d)$ con heurísticas.

\paragraph{Metaheurísticas (SA, TSH+PR)\\}

\textbf{Garantía:} Sin garantía teórica; calidad empírica esperada: 95--99\% del óptimo.

\textbf{Instancias donde son esenciales:}
\begin{itemize}
	\item Grafos grandes ($n > 15$) donde exactos no terminan
	\item Grafos densos ($d > 0.5$) con $\chi$ alto
	\item Cualquier instancia con $n \geq 20$
\end{itemize}

\textbf{Instancias donde son innecesarias:}
\begin{itemize}
	\item Árboles, paths, cycles: heurísticas encuentran óptimo
	\item Grafos pequeños ($n \leq 10$): BF es rápido
	\item Grafos de intervalo: DP es óptimo y rápido
\end{itemize}

\subsection{Límites de Escalabilidad de Fuerza Bruta}

\subsubsection{Análisis Teórico del Espacio de Búsqueda}

El espacio de búsqueda de Fuerza Bruta crece exponencialmente:

\begin{table}[H]
	\centering
	\begin{tabular}{ccccc}
		\toprule
		$n$ & $k=2$ & $k=3$ & $k=4$ & $k=5$ \\
		\midrule
		8 & 256 & 6,561 & 65,536 & 390,625 \\
		10 & 1,024 & 59,049 & 1,048,576 & 9,765,625 \\
		12 & 4,096 & 531,441 & 16,777,216 & 244,140,625 \\
		15 & 32,768 & 14,348,907 & 1,073,741,824 & 30,517,578,125 \\
		\bottomrule
	\end{tabular}
	\caption{Número de configuraciones a explorar: $k^n$}
\end{table}

\textbf{Tiempo estimado} (asumiendo $10^6$ configuraciones/segundo):

\begin{table}[H]
	\centering
	\begin{tabular}{ccccc}
		\toprule
		$n$ & $k=2$ & $k=3$ & $k=4$ & $k=5$ \\
		\midrule
		8 & $<0.01$s & 0.01s & 0.07s & 0.4s \\
		10 & 0.001s & 0.06s & 1.0s & 10s \\
		12 & 0.004s & 0.5s & 17s & 244s \\
		15 & 0.03s & 14s & 1074s & 8.5h \\
		\bottomrule
	\end{tabular}
	\caption{Tiempo estimado de ejecución para Fuerza Bruta}
\end{table}

\subsubsection{Límites Prácticos por Tipo de Instancia}

\begin{table}[H]
	\centering
	\begin{tabular}{lccc}
		\toprule
		\textbf{Tipo de Instancia} & \textbf{Densidad} & \textbf{$n_{\max}$ (BF)} & \textbf{Tiempo típico} \\
		\midrule
		Árboles & 0.00 & $n \leq 20$ ($k \leq 3$) & $< 10$s \\
		Paths / Cycles & 0.10--0.20 & $n \leq 20$ ($k \leq 3$) & $< 5$s \\
		Stars / Wheels & 0.10--0.40 & $n \leq 15$ ($k \leq 5$) & $< 60$s \\
		Grids & 0.15--0.30 & $n \leq 12$ ($k \leq 6$) & $< 60$s \\
		Erdős-Rényi $d=0.05$ & 0.05 & $n \leq 15$ ($k \leq 8$) & $< 60$s \\
		Erdős-Rényi $d=0.30$ & 0.30 & $n \leq 12$ ($k \leq 8$) & $< 180$s \\
		Erdős-Rényi $d=0.60$ & 0.60 & $n \leq 10$ ($k \leq 8$) & $< 60$s \\
		Completos $K_n$ & 1.00 & $n \leq 10$ ($k=n$) & $< 5$s \\
		Intervalo & 0.20--0.70 & $n \leq 12$ ($k \leq 6$) & $< 60$s \\
		Random (general) & Variable & $n \leq 10$--12 & $< 60$s \\
		\bottomrule
	\end{tabular}
	\caption{Límites de escalabilidad de Fuerza Bruta por tipo de instancia}
\end{table}

\textbf{Fórmula de decisión para viabilidad de BF:}

Para determinar si BF puede resolver una instancia en tiempo $T = 180$s:

\begin{equation}
	\text{Tiempo estimado} = k^n \cdot \frac{m}{10^6} \leq T
\end{equation}

donde $m = \frac{n(n-1)d}{2}$ es el número aproximado de aristas.

\textbf{Conclusión sobre límites:}
\begin{itemize}
	\item \textbf{Sparse} ($d < 0.15$): $n \leq 15$--20
	\item \textbf{Moderada} ($0.15 < d < 0.40$): $n \leq 10$--12
	\item \textbf{Densa} ($d > 0.40$): $n \leq 8$--10
	\item \textbf{Completa} ($d = 1.0$): $n \leq 10$
\end{itemize}

\subsection{Recomendaciones Algorítmicas por Instancia}

\subsubsection{Matriz de Decisión}

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{ccccc}
		\toprule
		\textbf{Tamaño} & \textbf{$\chi$ estimado} & \textbf{Estructura} & \textbf{Algoritmo Óptimo} & \textbf{Alternativa} \\
		\midrule
		$n \leq 8$ & Cualquiera & Cualquiera & BF & Backtracking \\
		$9 \leq n \leq 10$ & $\chi \leq 4$ & Sparse/Moderada & Backtracking & BF \\
		$9 \leq n \leq 10$ & $\chi > 4$ & Densa & ILP & Backtracking \\
		$11 \leq n \leq 12$ & Cualquiera & Sparse & Backtracking & ILP \\
		$11 \leq n \leq 12$ & $\chi \leq 4$ & Moderada & Backtracking & ILP \\
		$11 \leq n \leq 12$ & $\chi > 4$ & Densa & ILP & Metaheur. \\
		$13 \leq n \leq 15$ & $\chi \leq 3$ & Sparse & ILP/Backtracking & Heurísticas \\
		$13 \leq n \leq 15$ & $\chi > 3$ & Densa & ILP & Metaheur. \\
		$16 \leq n \leq 20$ & Cualquiera & Cualquiera & Metaheurísticas & ILP (t/o) \\
		$n > 20$ & Cualquiera & Cualquiera & Metaheurísticas & Heurísticas \\
		Árbol & $\chi \leq 2$ & Acíclica & DP Árbol / Heur. & BF ($n \leq 20$) \\
		Intervalo & Variable & Cordal especial & DP Intervalo & Heurísticas \\
		\bottomrule
	\end{tabular}
	\caption{Matriz de decisión algorítmica según características de la instancia}
\end{table}

\subsubsection{Recomendaciones Específicas por Categoría}

\paragraph{Instancias con Estructura Especial}

Para árboles, paths, cycles, stars y wheels:
\begin{itemize}
	\item \textbf{Recomendado:} Heurísticas (DSATUR, greedy) - encuentran óptimo en $O(n^2)$
	\item \textbf{Validación:} Comparar con BF para $n \leq 12$
	\item \textbf{NO usar:} ILP, Metaheurísticas (innecesario)
\end{itemize}

\paragraph{Grafos de Intervalo}

\begin{itemize}
	\item \textbf{MEJOR:} DP Interval Graph Solver - óptimo en $O(n^2 \log n)$
	\item \textbf{Aceleración:} 100--1000$\times$ más rápido que BF
	\item \textbf{Secundario:} BF (solo para validación)
\end{itemize}

\paragraph{Pequeñas Instancias Generales} ($n \leq 10$)

\begin{itemize}
	\item \textbf{MEJOR:} Backtracking Inteligente - garantiza óptimo, poda eficiente, tiempo $<1$s
	\item \textbf{Alternativa:} BF (más lento pero simple)
	\item \textbf{Análisis:} Comparar Heurísticas vs Exacto
\end{itemize}

\paragraph{Medianas Instancias Pseudo-Aleatorias} ($n=11$--15, densidad moderada)

\begin{itemize}
	\item \textbf{MEJOR:} Backtracking Inteligente + ILP
	\item Backtracking: hasta $n=15$
	\item ILP: hasta $n=20$ con timeout
	\item Tiempo: 1--60s
	\item \textbf{Metaheurísticas:} Para densidad alta
\end{itemize}

\paragraph{Densas/Completas} ($\chi$ alto)

\begin{itemize}
	\item \textbf{MEJOR:} ILP Solver + Backtracking Inteligente
	\item ILP: hasta $n=15$
	\item Backtracking: hasta $n=12$
	\item \textbf{Para $n > 15$:} Metaheurísticas (ASA, Hybrid)
\end{itemize}

\paragraph{Grandes Instancias} ($n > 15$)

\begin{itemize}
	\item \textbf{ÚNICO VIABLE:} Metaheurísticas
	\item Adaptive SA / Hybrid / Trajectory Search
	\item Tiempo: 0.1--10s
	\item Calidad: 95--99\% óptima (estimado)
	\item \textbf{Baseline:} Heurística constructiva (greedy)
\end{itemize}

\subsection{Comparación Empírica}

\subsubsection{Métricas de Evaluación}

Se evalúa cada algoritmo con base en:

\begin{itemize}
	\item \textbf{Calidad de solución}: ratio $R = Z_{\text{Alg}} / Z^*$
	\item \textbf{Tiempo de ejecución}: medido en segundos
	\item \textbf{Escalabilidad}: tamaño máximo $n$ resoluble en tiempo razonable
	\item \textbf{Sensibilidad a la densidad}: comportamiento según $d$
	\item \textbf{Sensibilidad a la distribución de costos}: comportamiento según estructura de $C$
\end{itemize}

\subsubsection{Tabla Resumen de Rendimiento}

\begin{table}[H]
	\centering
	\footnotesize
	\begin{tabular}{lccccc}
		\toprule
		\textbf{Categoría} & \textbf{\#Inst} & \textbf{$n$-rango} & \textbf{Mejor Algo} & \textbf{BF Viable?} & \textbf{Densidad} \\
		\midrule
		Especiales & 21 & 3--7 & Todos & Sí & 0.0--1.0 \\
		Árboles & 14 & 8--20 & Heurísticas/DP-Tree & Sí & 0.0 \\
		Paths & 9 & 10--20 & Heurísticas & Sí & 0.10--0.20 \\
		Cycles & 9 & 10--20 & Heurísticas & Sí & 0.11--0.22 \\
		Completos & 12 & 10--20 & ILP/Backtrack & $n \leq 10$ & 1.0 \\
		Grillas & 9 & 10--20 & Heurísticas & $n \leq 10$ & 0.16--0.29 \\
		Intervalo & 13 & 4--20 & DP-Interval & Sí & 0.23--0.70 \\
		Erdős-Rényi & 20 & 15--25 & Metaheur. & $n \leq 12$ & 0.05--0.60 \\
		Random & 50 & 8--16 & Heurísticas & $n \leq 12$ & Variable \\
		Especiales Struct & 31 & 5--20 & Heurísticas & $n \leq 10$ & 0.10--0.40 \\
		\bottomrule
	\end{tabular}
	\caption{Resumen de rendimiento por categoría de instancia}
\end{table}

\subsection{Entregables}

El repositorio incluye:

\begin{itemize}
	\item Implementaciones de todos los algoritmos descritos
	\item Generador de instancias con las 7 categorías
	\item Scripts de evaluación y visualización
	\item Conjunto completo de instancias de prueba
	\item Documentación completa en \texttt{README.md}
	\item Resultados experimentales y análisis comparativo
\end{itemize}

\subsection{Conclusiones del Análisis Experimental}

El análisis experimental confirma las predicciones teóricas:

\begin{itemize}
	\item \textbf{Fuerza Bruta:} Viable solo hasta $n=10$--12, confirmando la complejidad $O(k^n)$
	\item \textbf{Backtracking:} Extiende límite hasta $n=15$ mediante poda efectiva
	\item \textbf{DP Estructural:} Aceleración dramática (100--1000$\times$) en árboles e intervalos
	\item \textbf{Heurísticas:} Encuentran óptimo en estructuras simples, subóptimas en aleatorias
	\item \textbf{Metaheurísticas:} Única opción viable para $n > 15$, calidad empírica excelente
\end{itemize}

La estrategia óptima es multifacética: explotar estructura cuando existe (DP), usar exactos para validación ($n \leq 15$), y recurrir a metaheurísticas para instancias realistas grandes.


\printbibliography

\end{document}

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/150062830/405dd8ba-5245-491a-8918-5fe908cfb700/informe-copy.tex)
[2](http://annals.math.princeton.edu/2005/162-1/p08)
[3](http://ieeexplore.ieee.org/document/7354447/)
[4](http://epubs.siam.org/doi/10.1137/080721479)
[5](https://dl.acm.org/doi/10.1145/3188745.3188804)
[6](https://dl.acm.org/doi/10.1145/509907.509915)
[7](http://arxiv.org/pdf/2403.19680.pdf)
[8](https://arxiv.org/pdf/2307.05701.pdf)
[9](https://arxiv.org/pdf/1012.4701.pdf)
[10](http://arxiv.org/pdf/2403.18497.pdf)
[11](https://dmtcs.episciences.org/2136/pdf)
[12](https://arxiv.org/pdf/2205.08022.pdf)
[13](http://arxiv.org/pdf/2203.05887.pdf)
[14](http://arxiv.org/pdf/1910.01658.pdf)
[15](https://linkinghub.elsevier.com/retrieve/pii/S1571065317302731)
[16](https://arxiv.org/abs/1703.09726)
[17](https://drops.dagstuhl.de/opus/volltexte/2018/8530/pdf/LIPIcs-STACS-2018-10.pdf)
[18](http://arxiv.org/pdf/2308.00355.pdf)
[19](http://arxiv.org/pdf/2404.17011.pdf)
[20](http://arxiv.org/pdf/2311.10616.pdf)
[21](https://arxiv.org/pdf/1910.10364.pdf)
[22](https://arxiv.org/pdf/2410.19536.pdf)
[23](http://arxiv.org/pdf/2312.10114.pdf)
[24](http://link.springer.com/10.1007/s00453-020-00686-7)
[25](http://arxiv.org/pdf/0908.2375.pdf)
[26](http://arxiv.org/abs/0911.4218)
[27](http://arxiv.org/pdf/2504.04984.pdf)
[28](http://arxiv.org/pdf/2012.15056.pdf)
[29](https://dmtcs.episciences.org/570/pdf)
[30](https://arxiv.org/pdf/2109.05948.pdf)
[31](https://dmtcs.episciences.org/548/pdf)
[32](http://arxiv.org/pdf/1909.02261.pdf)
[33](http://arxiv.org/pdf/2402.09998.pdf)
[34](https://arxiv.org/pdf/2210.05915.pdf)
[35](https://www.aanda.org/10.1051/0004-6361/202554937)
[36](https://arxiv.org/pdf/2008.05374.pdf)
[37](http://arxiv.org/pdf/1809.06506.pdf)
[38](https://arxiv.org/pdf/1902.03702.pdf)
[39](http://arxiv.org/pdf/2102.01149.pdf)
[40](http://arxiv.org/pdf/2111.08100.pdf)
[41](https://arxiv.org/abs/2211.04444)
[42](http://arxiv.org/pdf/0906.1557.pdf)
[43](https://arxiv.org/pdf/1702.01836v1.pdf)
[44](http://arxiv.org/pdf/2401.03832.pdf)
[45](https://arxiv.org/pdf/1804.03197.pdf)
[46](https://arxiv.org/pdf/2101.06306.pdf)
[47](https://arxiv.org/abs/2502.15216)
[48](http://www.ijcir.com/publishedPapers.php?showDetails=Y&idArticle=116&volume=1&number=1&volume_id=2&number_id=4)
[49](https://proceedings.science/proceedings/100607/_papers/212538)
[50](https://www.semanticscholar.org/paper/4ba3abf4aeb57aafb75f943c958b365da659716c)
[51](https://www.mdpi.com/2227-7390/13/21/3421)
[52](https://www.nature.com/articles/s43588-024-00735-z)
[53](http://ojs.uma.ac.id/index.php/jime/article/view/2328)
[54](https://www.semanticscholar.org/paper/7317a1fe522bf2684a69e9d0e674a7efdf55f6d5)
[55](https://www.semanticscholar.org/paper/e386073b84552e20b1e04412dfa3df6fd78dc386)
[56](https://www.semanticscholar.org/paper/c54bfd5fde974027c4d44cde96d4965b35a1c849)
[57](http://rcin.org.pl/Content/139750/PDF/RB-2007-07.pdf)
[58](https://arxiv.org/pdf/1712.00709.pdf)
[59](http://arxiv.org/abs/2012.04470)
[60](https://link.aps.org/doi/10.1103/PhysRevX.13.021011)
[61](http://arxiv.org/pdf/2504.04821.pdf)
[62](http://arxiv.org/pdf/1903.07056.pdf)
[63](https://pmc.ncbi.nlm.nih.gov/articles/PMC3498173/)
[64](https://arxiv.org/pdf/2309.11961.pdf)
[65](https://www.rairo-ro.org/10.1051/ro/2019037)
[66](https://www.semanticscholar.org/paper/360f6d4e5a4ab2f5443ceaef17c03838815d1876)
[67](https://www.extrica.com/article/23228)
[68](https://www.worldscientific.com/doi/10.1142/S1793830922501488)
[69](https://ieeexplore.ieee.org/document/10231327/)
[70](https://www.degruyterbrill.com/document/doi/10.1515/math-2025-0193/html)
[71](http://www.aimspress.com/article/doi/10.3934/math.2021090)
[72](https://dl.acm.org/doi/10.1145/3605573.3605623)
[73](https://www.semanticscholar.org/paper/64ad02f6ce47747e469009c98a09070ebeaedf6e)
[74](http://link.springer.com/10.1007/s00500-019-04278-8)
[75](http://arxiv.org/pdf/1802.06742.pdf)
[76](https://arxiv.org/pdf/2109.06021.pdf)
[77](https://drops.dagstuhl.de/opus/volltexte/2018/8492/pdf/LIPIcs-STACS-2018-35.pdf)
[78](http://arxiv.org/pdf/2203.08885.pdf)
[79](https://www.mdpi.com/2073-8994/12/6/965/pdf)
[80](http://arxiv.org/pdf/2411.00679.pdf)
[81](https://arxiv.org/pdf/1907.05117.pdf)
[82](https://arxiv.org/pdf/2501.05796.pdf)
[83](https://www.cambridge.org/core/product/identifier/9781108637435\%23c2/type/book_part)
[84](https://link.springer.com/10.1007/978-3-319-21275-3)
[85](https://link.springer.com/10.1007/978-3-319-21275-3_5)
[86](https://dl.acm.org/doi/10.1145/3618260.3649656)
[87](http://link.springer.com/10.1007/978-3-642-28050-4_2)
[88](https://ojs.aaai.org/index.php/AAAI/article/view/10575)
[89](https://link.springer.com/10.1007/978-3-319-21275-3_9)
[90](https://www.semanticscholar.org/paper/18800b91619c3bfe1ab5c9c73c527f87ccb44c7c)
[91](https://link.springer.com/10.1007/978-3-319-21275-3_13)
[92](https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.STACS.2019.45)
[93](https://arxiv.org/pdf/1401.3492.pdf)
[94](http://arxiv.org/pdf/2501.14461.pdf)
[95](http://arxiv.org/pdf/2411.13171.pdf)
[96](https://arxiv.org/pdf/2310.03469.pdf)
[97](https://arxiv.org/pdf/2501.11544.pdf)
[98](https://arxiv.org/pdf/2210.02167.pdf)
[99](http://arxiv.org/pdf/2306.08812.pdf)
[100](http://arxiv.org/pdf/2207.07425.pdf)